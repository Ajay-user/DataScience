{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Clustering\n",
    "There are many types of clustering algorithms of which here are the top 4 well-known ones:\n",
    "\n",
    "* Connectivity-based Clustering\n",
    "* Centroid-based Clustering\n",
    "* Distribution-based Clustering\n",
    "* Density-based Clustering\n",
    "\n",
    "### Clustering Principles:\n",
    "All clustering algorithms try to group data points based on similarities between the data. What does this actually mean?\n",
    "\n",
    "\n",
    "It is often spoken of, in terms of **`inter-cluster heterogeneity`** and **`intra-cluster homogeneity`**. \n",
    "\n",
    "* `Inter-cluster heterogeneity`<br> This means that the clusters are as different from one another as possible. The characteristics of one cluster are very different from another cluster. This makes the clusters very stable and reliable.\n",
    "* `Intra-cluster homogeneity`<br> This talks about how similar are the characteristics of all the data within the cluster. The more similar, the more cohesive is the cluster and hence more stable. \n",
    "\n",
    "* **Hence the objective of clustering is to maximise the inter-cluster distance (Inter-cluster heterogeneity) and minimise the intra-cluster distance (intra-cluster homogeneity )**\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Density-Based Spatial Clustering of Applications with Noise\n",
    "\n",
    "Most of the traditional clustering algorithms like Centroid based Kmeans and Connectivity based Heirarchical can be used to group data in an unsupervised way. However when applied to tasks with arbitary shape clusters or clusters within clusters traditional clustering methods might not be able to acheive good results.\n",
    "\n",
    "For Example : Kmeans can cause problems in the domain of anomaly detection. Because Kmeans assign the anomaly to the same cluster as normal data. The anomaly pulls the cluster centroid towards them making it harder to classify the anomaly from data.\n",
    "* Kmeans algorithm has no notion of outliers\n",
    "    * Kmeans asssign all points to a cluster even if they dont belong to any\n",
    "* Density based clustering locates regions of high density, and seperates the outliers.\n",
    "    * Density in this context is the number of points within a specified radius\n",
    "*   DBSCAN can find dense cluster and seperate the noise\n",
    "\n",
    "**Idea** : *If a particular point belongs to a cluster, it should be near to lots of other points in that cluster*\n",
    "\n",
    "DBSCAN works based on two important parameters\n",
    "* Radius of neighbourhood (R)<br>\n",
    "The radius,`\"R\"`, defines an area that, if included enough number of points within, we call it a dense area\n",
    "* Minimum number of neighbours (M) <br>\n",
    "The `\"M\"` define the minimum number of points we want in a neighbourhood to define a cluster\n",
    "\n",
    "**Each point in our dataset can be either** \n",
    "* **Core point**<br>\n",
    "A data-point is a core-point if it has in its neighbourhood `\"M\"` data-points\n",
    "* **Border point**<br>\n",
    "A data-point is a border-point if it has less than `\"M\"` data-points in its neighbourhood<br> and is reachable from any of the core-points\n",
    "* **Outlier point**<br>\n",
    "A data-point is a border-point if it has less than `\"M\"` data-points in its neighbourhood<br> and is **not** reachable from any of the core-points\n",
    "\n",
    "**A cluster is formed by connecting all core-points that are in the neighbourhood along with the border-points which are reachable from those core-points**\n",
    "\n",
    "Advantages of DBSCAN\n",
    "* Robust to outliers\n",
    "* Does not require specification of the number of cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = [[1, 1], [-1, -1], [1, -1]]\n",
    "X, labels_true = make_blobs(\n",
    "    n_samples=750, centers=centers, cluster_std=0.4, random_state=0\n",
    ")\n",
    "\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1]);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./plots/data-3-blobs.png'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DBSCAN\n",
    "* `eps` <br>\n",
    " The maximum distance between two samples for one to be considered as in the neighborhood of the other.\n",
    "* `min_samples` <br>\n",
    "    The number of samples (or total weight) in a neighborhood for a point to be considered as a core point. This includes the point itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters found : 3 Number of noise found : 18\n"
     ]
    }
   ],
   "source": [
    "dbscan = DBSCAN(eps=0.3, min_samples=10, metric='minkowski', p=2)\n",
    "dbscan.fit(X)\n",
    "\n",
    "noise = dbscan.labels_==-1\n",
    "clusters_found = len(np.unique(dbscan.labels_[~noise]))\n",
    "print('Number of clusters found :',clusters_found,'Number of noise found :',len(dbscan.labels_[noise]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize clusters\n",
    "* The color indicates cluster membership, with large circles indicating core samples found by the algorithm. \n",
    "* Smaller circles are non-core samples that are still part of a cluster. \n",
    "* The outliers are indicated by black points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = np.array(['salmon','lightblue','seagreen'])\n",
    "core_samples = X[dbscan.core_sample_indices_]\n",
    "\n",
    "plt.scatter(X[~noise, 0], X[~noise, 1], color=colors[dbscan.labels_[~noise]], edgecolors='k')\n",
    "plt.scatter(\n",
    "    core_samples[:, 0], core_samples[:, 1], \n",
    "    color=colors[dbscan.labels_[dbscan.core_sample_indices_]],\n",
    "    edgecolors='k', s=90)\n",
    "plt.scatter(X[noise, 0], X[noise, 1], c='k')\n",
    "plt.title(f'Number of clusters found :{clusters_found} & Number of noise found {len(dbscan.labels_[noise])}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./plots/dbscan-result.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9dd57416d08487125cddc31714a1d5a29ab9aaf930e8420812b05ce6347e3520"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
