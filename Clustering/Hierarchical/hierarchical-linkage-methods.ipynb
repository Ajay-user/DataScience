{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Clustering\n",
    "There are many types of clustering algorithms of which here are the top 4 well-known ones:\n",
    "\n",
    "* Connectivity-based Clustering\n",
    "* Centroid-based Clustering\n",
    "* Distribution-based Clustering\n",
    "* Density-based Clustering\n",
    "\n",
    "### Clustering Principles:\n",
    "All clustering algorithms try to group data points based on similarities between the data. What does this actually mean?\n",
    "\n",
    "\n",
    "It is often spoken of, in terms of **`inter-cluster heterogeneity`** and **`intra-cluster homogeneity`**. \n",
    "\n",
    "* `Inter-cluster heterogeneity`<br> This means that the clusters are as different from one another as possible. The characteristics of one cluster are very different from another cluster. This makes the clusters very stable and reliable.\n",
    "* `Intra-cluster homogeneity`<br> This talks about how similar are the characteristics of all the data within the cluster. The more similar, the more cohesive is the cluster and hence more stable. \n",
    "\n",
    "* **Hence the objective of clustering is to maximise the inter-cluster distance (Inter-cluster heterogeneity) and minimise the intra-cluster distance (intra-cluster homogeneity )**\n",
    "\n",
    "## Hierarchical Clustering\n",
    "* Here the distance is calculated between points themselves and not any centroid. Hence it is called connectivity-based clustering algorithm. \n",
    "\n",
    "The process of Hierarchical Clustering can be acheived in two ways\n",
    "* Agglomerative <br>\n",
    " This is an iterative process in which the datapoints that are closest to each other keep getting fused till we get one large cluster containing all the points. This is called Agglomerative Clustering. This process follows bottom-up approach<br>\n",
    "* Divisive <br>\n",
    "This process follows Top-down approach where we start with a single cluster, then iterative divide into sub-clusters \n",
    "    * Start with all data as one cluster and iteratively break down to many clusters depending on similarity criteria\n",
    "\n",
    "#### This iterative process leads to the formation of a tree-like structure called the dendrogram. The height of the dendrogram is a measure of the dissimilarity between the clusters\n",
    "\n",
    "####  During both the types of hierarchical clustering, the distance between two sub-clusters needs to be computed. The different types of linkages describe the different approaches to measure the distance between two sub-clusters of data points. \n",
    "\n",
    "The different types of linkages are\n",
    "* Single linkage\n",
    "* Complete linkage\n",
    "* Average linkage\n",
    "* Ward linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import cluster, datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.image import grid_to_graph\n",
    "from itertools import cycle, islice\n",
    "\n",
    "from tempfile import mkdtemp\n",
    "from joblib import memory\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate datasets. \n",
    "* We choose the size big enough to see the scalability of the algorithms, but not too big to avoid too long running times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 1500\n",
    "\n",
    "# Make a large circle containing a smaller circle in 2d.\n",
    "noisy_circles = datasets.make_circles(n_samples=n_samples, factor=0.5, noise=0.05)\n",
    "# Make two interleaving half circles.\n",
    "noisy_moons = datasets.make_moons(n_samples=n_samples, noise=0.05)\n",
    "#  # Generate isotropic Gaussian blobs for clustering.\n",
    "blobs = datasets.make_blobs(n_samples=n_samples, random_state=8, centers=3)\n",
    "\n",
    "no_structure = np.random.rand(n_samples, 2), None\n",
    "\n",
    "# Anisotropicly distributed data\n",
    "random_state = 170\n",
    "X, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state)\n",
    "transformation = [[0.6, -0.6], [-0.4, 0.8]]\n",
    "X_aniso = np.dot(X, transformation)\n",
    "aniso = (X_aniso, y)\n",
    "\n",
    "# blobs with varied variances\n",
    "varied = datasets.make_blobs(\n",
    "    n_samples=n_samples, cluster_std=[1.0, 2.5, 0.5], random_state=random_state\n",
    ")\n",
    "\n",
    "\n",
    "data = {\n",
    "    'Noisy Circles': noisy_circles,\n",
    "    'Noisy Moons': noisy_moons,\n",
    "    'Blobs': blobs,\n",
    "    'No structure': no_structure,\n",
    "    'Anisotropic': aniso,\n",
    "    'Different Variance': varied\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_cluster(ax, data, n_cluster=2, linkage='ward', metric='euclidean', title='Agglomerative Clustering'):\n",
    "    \n",
    "    agglomerative_cluster = cluster.AgglomerativeClustering(\n",
    "        n_clusters=n_cluster, metric=metric, linkage=linkage)\n",
    "\n",
    "\n",
    "    x, y = data\n",
    "    agglomerative_cluster.fit(x, y)\n",
    "\n",
    "    colors = plt.cm.nipy_spectral(\n",
    "        (agglomerative_cluster.labels_.astype(float)/len(np.unique(agglomerative_cluster.labels_))))\n",
    "\n",
    "    ax.scatter(x[:, 0], x[:, 1], c=colors)\n",
    "    ax.set(title=f'{title}\\nLinkage :{linkage}, Metric :{metric}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single Linkage | uses the minimum of the distances between all observations of the two sets.\n",
    "\n",
    "<img src='./notes/single-linkage.PNG'>\n",
    "\n",
    "Single Linkage is the way of defining the distance between two clusters as the minimum distance between the members of the two clusters. \n",
    "\n",
    "If you calculate the pair-wise distance between every point in `cluster A` with every point in `cluster-B`, the smallest distance is taken as the distance between the clusters.\n",
    "\n",
    "This leads to the generation of very loose clusters which also means that the `intra-cluster variance` is very high. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=6, figsize=(20,4), constrained_layout=True)\n",
    "ax = ax.ravel()\n",
    "\n",
    "\n",
    "for i, (frame, (k, v)) in enumerate(zip(ax, data.items())):\n",
    "    n_cluster = 3 if k in ['Blobs', 'Different Variance', 'Anisotropic'] else 2\n",
    "    find_cluster(ax=frame, data=v, title=k, linkage='single', n_cluster=n_cluster)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./plots/single-linkage.png'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average linkage | uses the average of the distances of each observation of the two sets\n",
    "\n",
    "<img src='./notes/average-linkage.PNG'>\n",
    "\n",
    "In Average linkage, the distance between two clusters is the average of all distances between members of two clusters.<br> i.e the distance of a point from every other point in the other cluster is calculated and the average of all the distances is taken.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=6, figsize=(20,4), constrained_layout=True)\n",
    "ax = ax.ravel()\n",
    "\n",
    "\n",
    "for i, (frame, (k, v)) in enumerate(zip(ax, data.items())):\n",
    "    n_cluster = 3 if k in ['Blobs', 'Different Variance', 'Anisotropic'] else 2\n",
    "    find_cluster(ax=frame, data=v, title=k, linkage='average', n_cluster=n_cluster)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./plots/average-linkage.png'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete linkage | uses the maximum distances between all observations of the two sets\n",
    "\n",
    "<img src='./notes/complete-linkage.PNG'>\n",
    "\n",
    "In Complete Linkage, the distance between two clusters is defined by the maximum distance between the members of the two clusters. This leads to the generation of stable and close-knit clusters. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=6, figsize=(20,4), constrained_layout=True)\n",
    "ax = ax.ravel()\n",
    "\n",
    "\n",
    "for i, (frame, (k, v)) in enumerate(zip(ax, data.items())):\n",
    "    n_cluster = 3 if k in ['Blobs', 'Different Variance', 'Anisotropic'] else 2\n",
    "    find_cluster(ax=frame, data=v, title=k, linkage='complete', n_cluster=n_cluster)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./plots/complete-linkage.png'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ward linkage\n",
    "\n",
    "* Ward’s method is also known as Minimum variance method or Ward’s Minimum Variance Clustering Method\n",
    "* ‘ward’ minimizes the variance of the clusters being merged.\n",
    "* Ward linkage creates compact, even-sized clusters\n",
    "* If linkage is `“ward”`, only `“euclidean”` is accepted as metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=6, figsize=(20,4), constrained_layout=True)\n",
    "ax = ax.ravel()\n",
    "\n",
    "\n",
    "for i, (frame, (k, v)) in enumerate(zip(ax, data.items())):\n",
    "    n_cluster = 3 if k in ['Blobs', 'Different Variance', 'Anisotropic'] else 2\n",
    "    find_cluster(ax=frame, data=v, title=k, linkage='ward', n_cluster=n_cluster)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./plots/ward-linkage.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9dd57416d08487125cddc31714a1d5a29ab9aaf930e8420812b05ce6347e3520"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
