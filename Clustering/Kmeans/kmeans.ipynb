{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Clustering\n",
    "There are many types of clustering algorithms of which here are the top 4 well-known ones:\n",
    "\n",
    "* Connectivity-based Clustering\n",
    "* Centroid-based Clustering\n",
    "* Distribution-based Clustering\n",
    "* Density-based Clustering\n",
    "\n",
    "### Clustering Principles:\n",
    "All clustering algorithms try to group data points based on similarities between the data. What does this actually mean?\n",
    "\n",
    "\n",
    "It is often spoken of, in terms of **`inter-cluster heterogeneity`** and **`intra-cluster homogeneity`**. \n",
    "\n",
    "* `Inter-cluster heterogeneity`<br> This means that the clusters are as different from one another as possible. The characteristics of one cluster are very different from another cluster. This makes the clusters very stable and reliable.\n",
    "* `Intra-cluster homogeneity`<br> This talks about how similar are the characteristics of all the data within the cluster. The more similar, the more cohesive is the cluster and hence more stable. \n",
    "\n",
    "* **Hence the objective of clustering is to maximise the inter-cluster distance (Inter-cluster heterogeneity) and minimise the intra-cluster distance (intra-cluster homogeneity )**\n",
    "\n",
    "\n",
    "\n",
    "# K-Means Clustering\n",
    "\n",
    "### STEP-1\n",
    "*   #### Select the number of clusters you want to identify in your data\n",
    "*   #### This is the `k` in kmeans\n",
    "\n",
    "<img src='./notes/customer segmentation.png'>\n",
    "\n",
    "### STEP-2\n",
    "* #### Randomly select `k`  centroids -- cluster  centers\n",
    "* #### These are the initial clusters\n",
    "\n",
    "### STEP-3\n",
    "* #### Measure the distance between data-points and  cluster-centers\n",
    "* #### Use `Euclidean distance` for calculating the distance\n",
    "\n",
    "\n",
    "### STEP-4\n",
    "* #### Assign data-points to nearest cluster based on the computed distance\n",
    "\n",
    "\n",
    "### STEP-5\n",
    "* #### Calculate the mean of each cluster  [`UPDATING CLUSTER CENTROID`]\n",
    "* #### Now repeat step-3 and 4\n",
    "* #### Measure the distance between datapoints and cluster-mean\n",
    "* #### Assign data-points to nearest cluster based on the distance calculated\n",
    "* #### This process continues until the clusters dont actually change\n",
    "\n",
    "<img src='./notes/customer segmentation - 1.png'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kmeans are not that great \n",
    "* K-means produces unintuitive and possibly undesirable clusters.\n",
    "* Usually by eyeballing we (humans) can identify better clustering than kmeans\n",
    "* We can assess the quality of clustering by adding up the variation within each cluster\n",
    "* Since k-means can't see the best clustering it's only option is to keep track of these clusters and their total variance and do the whole process over again with different starting points \n",
    "* Kmeans outputs the clustering that has the lowest variation\n",
    "* **CAUTION :** Set the `k` wisely, kmeans will always find k clusters in the data \n",
    "* Kmeans put the data into the number of clusters we give it as input\n",
    "* Whereas the Heirarchical clustering, tells us, pairwise, what two things are most similar\n",
    "\n",
    "## If you tell it to give you 4 clusters, it'll give you four clusters\n",
    "<img src='./notes/customer segmentation - 2.png'>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do you figure out what value to use for `K`\n",
    "* Domain knowledge\n",
    "* Trail & Error \n",
    "    * Elbow plot <br>\n",
    "    You can find `k` by finding the elbow in elbow-plot<br>\n",
    "    X-axis : different values of `k`<br>\n",
    "    Y-axis : reduction in variation <br>\n",
    "    You can observe a huge reduction in variation when `k` = # clusters\n",
    "\n",
    "<img src='./notes/Elbow-plot.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "from scipy.linalg import eigh\n",
    "\n",
    "from sklearn.cluster import KMeans, kmeans_plusplus\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstration of k-means \n",
    "\n",
    "\n",
    "#### Data generation\n",
    "* The function `make_blobs` generates `isotropic (spherical) gaussian blobs`. \n",
    "\n",
    "* To obtain `anisotropic (elliptical) gaussian blobs` one has to define a linear transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 1500\n",
    "random_state = 170\n",
    "transformation = [[0.60834549, -0.63667341], [-0.40887718, 0.85253229]]\n",
    "\n",
    "# spherical blobs [1500, 2]\n",
    "X, y = make_blobs(n_samples=n_samples, random_state=random_state, n_features=2, centers=3)\n",
    "\n",
    "# elliptical blobs  [1500, 2] \n",
    "# matrix multiplication\n",
    "X_aniso = np.dot(X, transformation)  # Anisotropic blobs\n",
    "\n",
    "\n",
    "# Unequal variance\n",
    "X_varied, y_varied = make_blobs(\n",
    "    n_samples=n_samples, cluster_std=[1.0, 2.5, 0.5], random_state=random_state, n_features=2, centers=3\n",
    ")  \n",
    "\n",
    "\n",
    "# Unevenly sized blobs\n",
    "X_filtered = np.vstack(\n",
    "    (X[y == 0][:500], X[y == 1][:100], X[y == 2][:10])\n",
    ")  \n",
    "\n",
    "y_filtered = [0] * 500 + [1] * 100 + [2] * 10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"Mixture of Gaussian Blobs\" : (X, y),\n",
    "    \"Anisotropically Distributed Blobs\" : (X_aniso, y),\n",
    "    \"Unequal Variance\" : (X_varied, y_varied),\n",
    "    \"Unevenly Sized Blobs\" : (X_filtered, y_filtered)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(10,10))\n",
    "ax = ax.ravel()\n",
    "\n",
    "keys = list(data.keys())\n",
    "\n",
    "for i, frame in enumerate(ax):\n",
    "    points, labels = data[keys[i]]\n",
    "    frame.scatter(points[:,0], points[:,1], c=labels)\n",
    "    frame.set(title=keys[i])\n",
    "    frame.set_xticks([])\n",
    "    frame.set_yticks([])\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./plots/clusters.png'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do we set the `k` in kmeans ?\n",
    "#### How do we set the number of clusters or How do you figure out what value to use for `K`\n",
    "* If we ask for `k` clusters the Kmeans will put the data into `k` clusters \n",
    "* **CAUTION** : `Non-optimal number of clusters`:<br>\n",
    "In a real world setting there is no uniquely defined true number of clusters. An appropriate number of clusters has to be decided from data-based criteria and knowledge of the intended goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clusters(ax, title, X, y):\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set(title=title)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(nrows=3, ncols=2, figsize=(12, 6))\n",
    "ax = ax.ravel()\n",
    "\n",
    "plot_clusters(ax[0], 'True clustering', X, y)\n",
    "\n",
    "for i in [1,2,3,4,5]:\n",
    "    y_preds = KMeans(n_clusters=i, n_init='auto', random_state=random_state).fit_predict(X, y)\n",
    "    plot_clusters(ax[i], f'kmeans clustering with k={i}', X, y_preds)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./plots/kmeans-clustering-and-parameter-k.png'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible solutions : silhouette analysis on KMeans clustering\n",
    "* Selecting the number of clusters with silhouette analysis on KMeans clustering.\n",
    "\n",
    "A higher Silhouette Coefficient score relates to a model with better defined clusters. \n",
    "\n",
    "The Silhouette Coefficient is defined for each sample and is composed of two scores:\n",
    "* `a`: The mean distance between a sample and all other points in the same class.\n",
    "* `b`: The mean distance between a sample and all other points in the next nearest cluster.\n",
    "\n",
    "The Silhouette Coefficient s for a single sample is then given as:\n",
    "* `(b - a) / max(a, b)`\n",
    "\n",
    "The Silhouette Coefficient for a set of samples is given as the mean of the Silhouette Coefficient for each sample.\n",
    "\n",
    "#### Advantages\n",
    "* The score is bounded between -1 for incorrect clustering and +1 for highly dense clustering. Scores around zero indicate overlapping clusters.\n",
    "\n",
    "* The score is higher when clusters are dense and well separated, which relates to a standard concept of a cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def silhouette_plot(k, X, y):\n",
    "    y_preds = KMeans(n_clusters=k, n_init='auto', random_state=random_state).fit_predict(X, y)\n",
    "\n",
    "    score = silhouette_score(X, y_preds)\n",
    "\n",
    "    # score for each sample\n",
    "    sample_score = silhouette_samples(X, y_preds)\n",
    "\n",
    "    # style : adding indent between bars\n",
    "    y_low= 10\n",
    "    # style : generate colors\n",
    "    colors__for_truth = plt.cm.nipy_spectral(y.astype(float) / len(np.unique(y)))\n",
    "    colors__for_prediction = plt.cm.nipy_spectral(y_preds.astype(float) / k)\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(15, 4))\n",
    "    ax[0].scatter(X[:, 0], X[:, 1], c=colors__for_truth)\n",
    "    ax[0].set(title='True cluster')\n",
    "\n",
    "    ax[1].scatter(X[:, 0], X[:, 1], c=colors__for_prediction)\n",
    "    ax[1].set(title=f'Kmeans clustering with k = {k}')\n",
    "\n",
    "    for cluster in range(k):\n",
    "        score_of_samples_in_the_cluster = sample_score[y_preds == cluster]\n",
    "        # sort the score\n",
    "        score_of_samples_in_the_cluster.sort()\n",
    "\n",
    "        # size of cluster\n",
    "        cluster_size = score_of_samples_in_the_cluster.shape[0]\n",
    "        # style : adding indent between bars\n",
    "        y_high = cluster_size + y_low\n",
    "\n",
    "        # style : generate colors\n",
    "        color = plt.cm.nipy_spectral(cluster/k)\n",
    "\n",
    "        y_axis = np.arange(y_low, y_high)\n",
    "        ax[2].fill_betweenx(y_axis, 0, score_of_samples_in_the_cluster, facecolor=color)\n",
    "\n",
    "        # style : adding indent between bars\n",
    "        y_low = y_high + 10\n",
    "\n",
    "        ax[2].set(title=f'Silhouette analysis\\nscore this clustering = {score:0.2f}')\n",
    "\n",
    "    # Silhouette score for the samples \n",
    "    ax[2].axvline(x=score, c='r')\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### When `k` = 2 , the obtained clusters have scores above average score\n",
    "### From the thickness of the silhouette plot the cluster size can be visualized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_plot(2, X, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./plots/silhouette_plot_kmeans_k_2.png'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When `k` = 3 , all the plots are more or less of similar thickness \n",
    "### Cluster scores are above the average \n",
    "### Looks perfect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_plot(3, X, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./plots/silhouette_plot_kmeans_k_3.png'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `k`=4 is a bad pick for the given data due to the presence of clusters with below average silhouette scores and also due to wide fluctuations in the size of the silhouette plots. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_plot(4, X, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./plots/silhouette_plot_kmeans_k_4.png'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When `k`=5 , there is wide fluctuations in the size of the silhouette plots but all cluster have score above the average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_plot(5, X, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./plots/silhouette_plot_kmeans_k_5.png'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Anisotropically Distributed Blobs\"  | `elliptical gaussian blobs`\n",
    "k-means consists of minimizing sample’s euclidean distances to the centroid of the cluster they are assigned to. As a consequence, **k-means is more appropriate for clusters that are isotropic and normally distributed** (i.e. `spherical gaussians`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = KMeans(n_clusters=3, n_init='auto', random_state=random_state).fit_predict(X_aniso, y)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
    "ax = ax.ravel()\n",
    "\n",
    "plot_clusters(ax[0], 'The data is elliptical gaussian distributed', X_aniso, y)\n",
    "\n",
    "plot_clusters(ax[1], 'Kmeans algorithm is appropriate for spherical gaussians\\nits not appropriate to use here', X_aniso, y_preds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./plots/kmeans-are-appropriate-for-spherical-gaussian-not-for-elliptical-gaussian.png'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anisotropic and unequal variances are real limitations of the k-means algorithm\n",
    "\n",
    "So instead of kmeans lets use  GaussianMixture, which also assumes gaussian clusters but does not impose any constraints on their variances. Notice that one still has to find the correct number of blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss_mix =  GaussianMixture(n_components=3, covariance_type='full')\n",
    "y_preds = gauss_mix.fit_predict(X_aniso, y)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
    "ax = ax.ravel()\n",
    "plot_clusters(ax[0], 'The data is elliptical gaussian distributed', X_aniso, y)\n",
    "plot_clusters(ax[1], 'GaussianMixture', X_aniso, y_preds)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./plots/anisotropic_unequal_variance_and_gauss_mix.png'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kmeans assumes cluster are spherical gaussian with same variance\n",
    "* `Unequal variance`: k-means is equivalent to taking the maximum likelihood estimator for a “mixture” of k gaussian distributions with the **same variances but with possibly different means**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = KMeans(n_clusters=3, n_init='auto', random_state=random_state).fit_predict(X_varied, y_varied)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
    "ax = ax.ravel()\n",
    "\n",
    "plot_clusters(ax[0], 'Clusters have different variance', X_varied, y_varied)\n",
    "\n",
    "plot_clusters(ax[1], 'Variance of clusters are not same\\nHence kmeans clustering will not be robust', X_varied, y_preds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./plots/clusters-have-different-variance-hence-kmeans-clustering-will-not-be-robust.png'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Several runs are recommended for sparse high-dimensional problems\n",
    "\n",
    "* `Unevenly sized blobs` <br> there is no theoretical result about k-means that states that it requires similar cluster sizes to perform well, yet **minimizing euclidean distances does mean that the more sparse and high-dimensional the problem is, the higher is the need to run the algorithm with different centroid seeds to ensure a global minimal inertia**.\n",
    "\n",
    "* `n_init` = ‘`auto`’ , default=10\n",
    "Number of times the k-means algorithm is run with different centroid seeds. The final results is the best output of n_init consecutive runs in terms of inertia. Several runs are recommended for sparse high-dimensional problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig, ax = plt.subplots(nrows=3, ncols=2, figsize=(12, 8))\n",
    "ax = ax.ravel()\n",
    "\n",
    "plot_clusters(ax[0], 'Clusters have different size', X_filtered, y_filtered)\n",
    "\n",
    "for i, j in enumerate([2, 5, 8, 10, 15]):\n",
    "    y_preds = KMeans(n_clusters=3, n_init=j, random_state=random_state).fit_predict(X_filtered, y_filtered)\n",
    "    plot_clusters(ax[i+1], f'kmeans with n_init={j}', X_filtered, y_preds)\n",
    "\n",
    "fig.suptitle('Several runs are recommended for sparse high-dimensional problems', size=16)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./plots/Several-runs-are-recommended-for-sparse-high-dim-data.png'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To deal with unevenly sized blobs one can increase the number of random initializations. \n",
    "In this case we increase the n_init to avoid finding a sub-optimal local minimum. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_init: 1 , Number of elements assigned to each cluster: [249 110 251]\n",
      "n_init: 2 , Number of elements assigned to each cluster: [249 110 251]\n",
      "n_init: 3 , Number of elements assigned to each cluster: [249 110 251]\n",
      "n_init: 4 , Number of elements assigned to each cluster: [249 110 251]\n",
      "n_init: 5 , Number of elements assigned to each cluster: [249 110 251]\n",
      "n_init: 6 , Number of elements assigned to each cluster: [500 100  10]\n",
      "n_init: 7 , Number of elements assigned to each cluster: [500 100  10]\n",
      "n_init: 8 , Number of elements assigned to each cluster: [500 100  10]\n",
      "n_init: 9 , Number of elements assigned to each cluster: [500 100  10]\n",
      "n_init: 10 , Number of elements assigned to each cluster: [500 100  10]\n",
      "True number of elements assigned to each cluster : [500 100  10]\n"
     ]
    }
   ],
   "source": [
    "for i in [ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]:\n",
    "    y_preds = KMeans(n_clusters=3, n_init=i, random_state=random_state).fit_predict(X_filtered, y_filtered)\n",
    "    n , size = np.unique(y_preds, return_counts=True)\n",
    "    print(f'n_init: {i} , Number of elements assigned to each cluster: {size}')\n",
    "\n",
    "n, true_size = np.unique(y_filtered, return_counts=True)\n",
    "print(f'True number of elements assigned to each cluster : {true_size}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* If we ask for `k` clusters the Kmeans will put the data into `k` clusters \n",
    "* **CAUTION** : `Non-optimal number of clusters`:<br>\n",
    "In a real world setting there is no uniquely defined true number of clusters. An appropriate number of clusters has to be decided from data-based criteria and knowledge of the intended goal.\n",
    "\n",
    "* `Anisotropically distributed blobs`: k-means consists of minimizing sample’s euclidean distances to the centroid of the cluster they are assigned to. As a consequence, k-means is more appropriate for clusters that are isotropic and normally distributed (i.e. spherical gaussians).\n",
    "\n",
    "* `Unequal variance`: k-means is equivalent to taking the maximum likelihood estimator for a “mixture” of k gaussian distributions with the **same variances but with possibly different means**.\n",
    "\n",
    "* `Unevenly sized blobs`: there is no theoretical result about k-means that states that it requires similar cluster sizes to perform well, yet **minimizing euclidean distances does mean that the more sparse and high-dimensional the problem is, the higher is the need to run the algorithm with different centroid seeds to ensure a global minimal inertia**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In high-dimensional spaces, Euclidean distances tend to become inflated . Running a dimensionality reduction algorithm prior to k-means clustering can alleviate this problem and speed up the computations\n",
    "\n",
    "#### In the case where clusters are known to be isotropic, have similar variance and are not too sparse, the k-means algorithm is quite effective and is one of the fastest clustering algorithms available. \n",
    "\n",
    "#### This advantage is lost if one has to restart it several times to avoid convergence to a local minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9dd57416d08487125cddc31714a1d5a29ab9aaf930e8420812b05ce6347e3520"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
