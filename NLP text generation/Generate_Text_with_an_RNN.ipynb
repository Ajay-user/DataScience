{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Generate Text with an RNN.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOPeG7TfobzY83fT9oYtBvs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ajay-user/ML-DL-RL-repo/blob/master/NLP%20text%20generation/Generate_Text_with_an_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QXAK7hPhLyL5"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import pathlib\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data ⛽"
      ],
      "metadata": {
        "id": "URqM0XK9Mpu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt'\n",
        "\n",
        "path_to_file = tf.keras.utils.get_file(origin=url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjQ99BNIL5Gl",
        "outputId": "eda42a0e-7cb1-4321-b260-ff6ec69bb483"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n",
            "1130496/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "txt_file = pathlib.Path(path_to_file)\n",
        "txt = txt_file.read_text('utf-8')\n",
        "print(txt[:500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQ-qNkYxMKv0",
        "outputId": "587a8a61-b0f5-4d99-e735-8b10f4ae4e35"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vocab  💬"
      ],
      "metadata": {
        "id": "vAujf_gUMsNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Total characters in file :',len(txt))\n",
        "\n",
        "vocab = set(txt)\n",
        "\n",
        "print('vocab size :',len(vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUARF_qmMMs2",
        "outputId": "30cb99c2-b148-47b7-c762-ab18a0606607"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total characters in file : 1115394\n",
            "vocab size : 65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mapping char to tokens 📘"
      ],
      "metadata": {
        "id": "COV9utnPM95a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "char_to_token = tf.keras.layers.StringLookup(vocabulary=list(vocab), mask_token=None)\n",
        "print('Size of vocab :',char_to_token.vocabulary_size())\n",
        "print(char_to_token.get_vocabulary()[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDm0gu0NNyBT",
        "outputId": "7652f064-223b-4794-9eb3-9bbca8e029a9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of vocab : 66\n",
            "['[UNK]', 'e', 'Y', 'G', 'u']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lets see an example\n",
        "sample_text = 'tensorflow'\n",
        "sample_chars = tf.strings.unicode_split(sample_text,'UTF-8')\n",
        "print('characters',sample_chars)\n",
        "sample_tokens = char_to_token(sample_chars)\n",
        "print('tokens :',sample_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbUnpC1XM11i",
        "outputId": "05684ee5-c3f4-4c90-e01f-a6f47697bda1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "characters tf.Tensor([b't' b'e' b'n' b's' b'o' b'r' b'f' b'l' b'o' b'w'], shape=(10,), dtype=string)\n",
            "tokens : tf.Tensor([ 8  1 56 11 21 30 36 37 21 47], shape=(10,), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mapping tokens to chars 📗"
      ],
      "metadata": {
        "id": "QU1w9Vb3O9b4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_to_char = tf.keras.layers.StringLookup(vocabulary=char_to_token.get_vocabulary(), invert=True, mask_token=None)\n",
        "print('Size of vocab :',token_to_char.vocabulary_size())\n",
        "print(token_to_char.get_vocabulary()[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4Az0EHLNTbi",
        "outputId": "595759db-4963-4355-ccda-e72fc57bed52"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of vocab : 66\n",
            "['[UNK]', 'e', 'Y', 'G', 'u']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('tokens :',sample_tokens)\n",
        "sample_tokens_to_char = token_to_char(sample_tokens)\n",
        "print(sample_tokens_to_char)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4ymD11pPQxh",
        "outputId": "a87eb48c-f9c3-401b-f849-0a97ac8f7dcc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokens : tf.Tensor([ 8  1 56 11 21 30 36 37 21 47], shape=(10,), dtype=int64)\n",
            "tf.Tensor([b't' b'e' b'n' b's' b'o' b'r' b'f' b'l' b'o' b'w'], shape=(10,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Util for printing\n",
        "def text_from_tokens(tokens):\n",
        "  return tf.strings.join(token_to_char(tokens))"
      ],
      "metadata": {
        "id": "3HtBhsmuUptx"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset 📥"
      ],
      "metadata": {
        "id": "5v-R5j6iRy7M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "characters = tf.strings.unicode_split(txt, 'UTF-8')\n",
        "tokens = char_to_token(characters)\n",
        "tokens_ds = tf.data.Dataset.from_tensor_slices(tokens)\n",
        "\n",
        "# lets use 100-characters as inputs \n",
        "tokens_ds = tokens_ds.batch(batch_size=101, drop_remainder=True)\n",
        "\n",
        "for toks in tokens_ds.take(2):\n",
        "  print(tf.strings.join(token_to_char(toks)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPzaJ_lRSiRd",
        "outputId": "494025d3-4674-4dd0-c90e-40d95367266f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou ', shape=(), dtype=string)\n",
            "tf.Tensor(b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k', shape=(), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inputs and Targets 🎯"
      ],
      "metadata": {
        "id": "QwoK_3akVKfh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_inputs_and_targets(sequence):\n",
        "  inp = sequence[:-1]\n",
        "  tar = sequence[1:]\n",
        "  return inp, tar"
      ],
      "metadata": {
        "id": "BMYgYxsnVA_I"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BATCH SIZE \n",
        "BATCH_SIZE = 64\n",
        "\n",
        "ds = (tokens_ds.map(get_inputs_and_targets)\n",
        "               .shuffle(10000)\n",
        "               .batch(BATCH_SIZE)\n",
        "               .prefetch(tf.data.AUTOTUNE)\n",
        "              )"
      ],
      "metadata": {
        "id": "UnpXGJSYVA5f"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_batch, y_batch = next(iter(ds))\n",
        "\n",
        "for X, y in zip(X_batch, y_batch):\n",
        "  print(text_from_tokens(X))\n",
        "  print(text_from_tokens(y))\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "suRB0SA5XGvk",
        "outputId": "9ffa3040-26b2-4bad-cf7b-88ed6adc81c6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(b\":\\nSo come to you and yours, as to this Prince!\\n\\nKING EDWARD IV:\\nWhere's Richard gone?\\n\\nCLARENCE:\\nTo \", shape=(), dtype=string)\n",
            "tf.Tensor(b\"\\nSo come to you and yours, as to this Prince!\\n\\nKING EDWARD IV:\\nWhere's Richard gone?\\n\\nCLARENCE:\\nTo L\", shape=(), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 🤖 \n"
      ],
      "metadata": {
        "id": "9zFmMmHbPrXp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE = char_to_token.vocabulary_size()\n",
        "EMB_DIMS = 256\n",
        "RNN_UNITS = 1024"
      ],
      "metadata": {
        "id": "QcgohCuGX3BH"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CharModel(tf.keras.layers.Layer):\n",
        "  def __init__(self,):\n",
        "    super(CharModel, self).__init__()\n",
        "    self.embedding = tf.keras.layers.Embedding(VOCAB_SIZE, EMB_DIMS)\n",
        "    self.gru = tf.keras.layers.GRU(RNN_UNITS, return_sequences=True, return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(VOCAB_SIZE)\n",
        "\n",
        "  def call(self, inputs, state):\n",
        "    # [batch, seq] --> [batch, seq, emb_dims]\n",
        "    embed = self.embedding(inputs)\n",
        "    \n",
        "    if state is None:\n",
        "      state = self.gru.get_initial_state(embed)\n",
        "    # [batch, seq, emb_dims] --> [batch, seq, rnn_units], [batch, rnn_units]\n",
        "    rnn, state = self.gru(embed, initial_state=state)\n",
        "\n",
        "    # [batch, seq, rnn_units] --> [batch, seq, vocab_size]\n",
        "    out = self.dense(rnn)\n",
        "\n",
        "    return out, state\n"
      ],
      "metadata": {
        "id": "5DglR5FpPlLv"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets test the model\n",
        "charModel = CharModel()\n",
        "out, state = charModel(X_batch, state=None)\n",
        "print('Input shape :',X_batch.shape)\n",
        "print('RNN output vector shape :',out.shape)\n",
        "print('RNN state vector shape :',state.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JHnAOd5P2RL",
        "outputId": "f394e4b1-a245-4e0a-f896-efd73fc5317b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape : (64, 100)\n",
            "RNN output vector shape : (64, 100, 66)\n",
            "RNN state vector shape : (64, 1024)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer(tf.keras.Model):\n",
        "  def __init__(self,):\n",
        "    super(Trainer, self).__init__()\n",
        "    self.model = CharModel()\n",
        "\n",
        "  @tf.function\n",
        "  def train_step(self, inputs, state=None):\n",
        "    X, y = inputs\n",
        "    with tf.GradientTape() as tape:\n",
        "      preds, state = self.model(X, state)\n",
        "      loss = self.loss(y, preds)\n",
        "    gradients = tape.gradient(loss, self.trainable_variables)\n",
        "    self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "    return {'loss':loss}"
      ],
      "metadata": {
        "id": "9FWPCPBucde-"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer()\n",
        "\n",
        "trainer.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "0iNr1Z3kiKUJ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's see this in action\n",
        "trainer.train_step(next(iter(ds)), state=None)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bty8TfGyiTkI",
        "outputId": "0aa3b230-15eb-4467-f85b-39ccffab478a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'loss': <tf.Tensor: shape=(), dtype=float32, numpy=4.1889133>}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# training \n",
        "trainer.fit(ds, epochs=30)"
      ],
      "metadata": {
        "id": "qtHMZwN4jXuk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74d24475-1f5d-4a12-c523-3b7fb7566570"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "173/173 [==============================] - 11s 51ms/step - loss: 2.7139\n",
            "Epoch 2/30\n",
            "173/173 [==============================] - 10s 51ms/step - loss: 1.9907\n",
            "Epoch 3/30\n",
            "173/173 [==============================] - 10s 52ms/step - loss: 1.7164\n",
            "Epoch 4/30\n",
            "173/173 [==============================] - 10s 52ms/step - loss: 1.5564\n",
            "Epoch 5/30\n",
            "173/173 [==============================] - 10s 53ms/step - loss: 1.4567\n",
            "Epoch 6/30\n",
            "173/173 [==============================] - 11s 54ms/step - loss: 1.3863\n",
            "Epoch 7/30\n",
            "173/173 [==============================] - 11s 55ms/step - loss: 1.3339\n",
            "Epoch 8/30\n",
            "173/173 [==============================] - 11s 55ms/step - loss: 1.2870\n",
            "Epoch 9/30\n",
            "173/173 [==============================] - 11s 54ms/step - loss: 1.2467\n",
            "Epoch 10/30\n",
            "173/173 [==============================] - 11s 54ms/step - loss: 1.2075\n",
            "Epoch 11/30\n",
            "173/173 [==============================] - 11s 55ms/step - loss: 1.1671\n",
            "Epoch 12/30\n",
            "173/173 [==============================] - 10s 54ms/step - loss: 1.1267\n",
            "Epoch 13/30\n",
            "173/173 [==============================] - 11s 54ms/step - loss: 1.0836\n",
            "Epoch 14/30\n",
            "173/173 [==============================] - 11s 55ms/step - loss: 1.0379\n",
            "Epoch 15/30\n",
            "173/173 [==============================] - 11s 54ms/step - loss: 0.9907\n",
            "Epoch 16/30\n",
            "173/173 [==============================] - 11s 54ms/step - loss: 0.9397\n",
            "Epoch 17/30\n",
            "173/173 [==============================] - 11s 55ms/step - loss: 0.8873\n",
            "Epoch 18/30\n",
            "173/173 [==============================] - 11s 55ms/step - loss: 0.8348\n",
            "Epoch 19/30\n",
            "173/173 [==============================] - 11s 55ms/step - loss: 0.7833\n",
            "Epoch 20/30\n",
            "173/173 [==============================] - 11s 54ms/step - loss: 0.7328\n",
            "Epoch 21/30\n",
            "173/173 [==============================] - 11s 55ms/step - loss: 0.6863\n",
            "Epoch 22/30\n",
            "173/173 [==============================] - 11s 55ms/step - loss: 0.6462\n",
            "Epoch 23/30\n",
            "173/173 [==============================] - 11s 54ms/step - loss: 0.6098\n",
            "Epoch 24/30\n",
            "173/173 [==============================] - 10s 54ms/step - loss: 0.5775\n",
            "Epoch 25/30\n",
            "173/173 [==============================] - 10s 54ms/step - loss: 0.5516\n",
            "Epoch 26/30\n",
            "173/173 [==============================] - 10s 54ms/step - loss: 0.5292\n",
            "Epoch 27/30\n",
            "173/173 [==============================] - 10s 54ms/step - loss: 0.5120\n",
            "Epoch 28/30\n",
            "173/173 [==============================] - 11s 55ms/step - loss: 0.4931\n",
            "Epoch 29/30\n",
            "173/173 [==============================] - 11s 55ms/step - loss: 0.4790\n",
            "Epoch 30/30\n",
            "173/173 [==============================] - 11s 54ms/step - loss: 0.4674\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f631033ecd0>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Text 📧"
      ],
      "metadata": {
        "id": "xcbUJ5CaSgyl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# lets see how to get output tokens from logits\n",
        "logits, state = charModel(X_batch, state=None)\n",
        "# shape of logits = [batch, seq, vocab_size]\n",
        "# lets take the last char\n",
        "char = logits[:,-1, :] #[batch, vocab_size]\n",
        "# now lets take a sample \n",
        "sample = tf.random.categorical(char, num_samples=1) #shape [batch, num_samples]\n",
        "print('Shape of sample', sample.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvFpQKdJY8Dj",
        "outputId": "53156994-4651-4e59-8f21-adef044a69d5"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of sample (64, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GenerateText(tf.Module):\n",
        "  def __init__(self, model, char_to_token, token_to_char,window_size):\n",
        "    super(GenerateText, self).__init__()\n",
        "    self.model = model\n",
        "    self.char_to_token = char_to_token\n",
        "    self.token_to_char = token_to_char\n",
        "    self.window_size = window_size\n",
        "\n",
        "  def process_inputs(self, inputs):\n",
        "    token_array = tf.TensorArray(dtype=tf.int64, size=1, dynamic_size=True)\n",
        "    ragged = self.char_to_token(tf.strings.unicode_split(inputs,'UTF-8'))\n",
        "    for i,tensor in enumerate(ragged):\n",
        "      token_array = token_array.write(i,tensor[-self.window_size:])\n",
        "    return token_array.stack()\n",
        "  \n",
        "  def sample(self, logits):\n",
        "    last = logits[:,-1, :]\n",
        "    return tf.random.categorical(last, num_samples=1)\n",
        "\n",
        "  def __call__(self, inputs, state, n_iter=1000):\n",
        "    # [text batch] --> [batch, window_size]\n",
        "    input_tokens = self.process_inputs(inputs)\n",
        "    output_tokens = tf.TensorArray(dtype=tf.int64, size=1, dynamic_size=True)\n",
        "    \n",
        "    for i in range(n_iter):\n",
        "      logits, state = self.model(input_tokens, state)\n",
        "      sample = self.sample(logits) # [batch, 1]\n",
        "      output_tokens = output_tokens.write(i, sample)\n",
        "      input_tokens = tf.concat([input_tokens, sample], axis=1) # [batch, window_size + 1]\n",
        "      input_tokens = input_tokens[:, -self.window_size:]  # [batch, window_size]\n",
        "    \n",
        "    # [n_iter, batch, 1] -->  [n_iter, batch]\n",
        "    output_tokens = tf.squeeze(output_tokens.stack())  \n",
        "    # [n_iter, batch] --> [batch, n_iter]\n",
        "    output_tokens = tf.transpose(output_tokens)\n",
        "    # text from tokens\n",
        "    output_chars = self.token_to_char(output_tokens)\n",
        "    outputs = tf.strings.reduce_join(output_chars, axis=1)\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "R5OXBFPaSO2b"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lets generate Shakespeare's writing ✍"
      ],
      "metadata": {
        "id": "71JzGuzfC-mx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_inputs = [\n",
        "    \"\"\"ROMEO:\n",
        "Why, sir, what think you, sir?,\"\"\",\n",
        "\n",
        " \"\"\"Caius Marcius is chief enemy to the people.\"\"\" ,\n",
        "\n",
        " \"\"\"All:\n",
        "No more talking on't; let it be done: away, away!\"\"\"  ,\n",
        "\n",
        "\"\"\"\n",
        "First Citizen:\n",
        "Before we proceed any further, hear me speak.\n",
        "\"\"\"\n",
        "]"
      ],
      "metadata": {
        "id": "o_2vz5XF5ac-"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Window size = 1 👓"
      ],
      "metadata": {
        "id": "xyWFyPDsDKd4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text = GenerateText(trainer.model, char_to_token, token_to_char, window_size=1)"
      ],
      "metadata": {
        "id": "vOgrug4Y75kz"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_outputs = generate_text(sample_inputs, state=None)"
      ],
      "metadata": {
        "id": "I9HdGRTF5yhU"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let see a sample\n",
        "print(sample_inputs[2]+(generated_outputs[2].numpy()).decode())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JR8EAM-5yOe",
        "outputId": "72982497-6690-4d3c-f1dc-7685b18a7f75"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "RIVERS:\n",
            "Go; away, my lord.\n",
            "\n",
            "LEONTES:\n",
            "They choose but speed;\n",
            "Come, go with me; and set up his feitness\n",
            "to death, and suffer earth and wive of death:\n",
            "He read the gods for her accounted him, not how to cure this case,\n",
            "To unwith death hath neither stolen.\n",
            "\n",
            "WARWICK:\n",
            "Dispate not with her, then, if any gentleman, I\n",
            "betraying is our guiltless traitor to the Duke of York.\n",
            "\n",
            "YORK:\n",
            "Apprehensio, the devil is too cold from whenly\n",
            "Upon thy woes which namest that vain. But, seath, let him\n",
            "along,--\n",
            "\n",
            "First Citizen:\n",
            "Among our parliament, then, the tyrant's revenge!\n",
            "\n",
            "Second Senator:\n",
            "She whereby thou wert keep me to their will.\n",
            "\n",
            "KING RICHARD II:\n",
            "Well go vinging them, but thine away;\n",
            "And be it not poison need, that may bed,\n",
            "Whom I unnaturally clouds, condemn'd by him.\n",
            "He'll not speak a little from you me your brows.\n",
            "But might I had rather\n",
            "You know not what you truly: why, 'madam,' and is not care of\n",
            "the people, beggarity, or in here of all the city\n",
            "Is ricelon than the Tyrrel, embraces mad like twell: good\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Util for printing generated text 🥡"
      ],
      "metadata": {
        "id": "4NOSNdHvFJT1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text_util(window_size, print_index):\n",
        "  generate_text = GenerateText(trainer.model, char_to_token, token_to_char, window_size=window_size)\n",
        "  generated_outputs = generate_text(sample_inputs, state=None)\n",
        "\n",
        "  text = []\n",
        "  for i, o in zip(sample_inputs, generated_outputs):\n",
        "    text.append(i+(o.numpy()).decode())\n",
        "\n",
        "  print(text[print_index])"
      ],
      "metadata": {
        "id": "yU3ZQx89DjoP"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Window size 30 ⛳ "
      ],
      "metadata": {
        "id": "yqzQHnufFlAr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text_util(window_size=30, print_index=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OvqbL24Eg_t",
        "outputId": "a9c620ec-04f1-46a6-df69-0f0563b3d8d9"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "ROMEO:\n",
            "My lords, it ellease it, I warrant him.\n",
            "\n",
            "LARD FITZABETH:\n",
            "With all my heart; and be it poss\n",
            "The nightful kings and to devour\n",
            "My daughter and serve to mend my brother;\n",
            "Or, if my kinsman, wrang me not\n",
            "To say 'Beven in the lukewarm pleasant,\n",
            "An twenty men, employ'd and Harl\n",
            "Witumbrader than a man upon thim,\n",
            "Would I weigh unto the battle's\n",
            "wife with nobleness; they are great\n",
            "Apprehion' ganes: the fool, is a\n",
            "good time.\n",
            "\n",
            "POMPEY:\n",
            "Trito, I can relaye to make thee gall.\n",
            "\n",
            "TRANIO:\n",
            "Why, you have had your trien uncoppunned\n",
            "with the nobles. You are to blame\n",
            "My father 's,and, reply nice and death?\n",
            "See, to my friends, for our King,\n",
            "Hath sworn to me again wor here,\n",
            "Ay, but for slaves, I have a stand retire:\n",
            "Again, a pack of young womb and other murdered.\n",
            "Who dost thou mean with those than sea-son?\n",
            "Then, give not this, my daughter\n",
            "A bury starved die thy heels;\n",
            "And byrecting on their summers:\n",
            "Juliet, then, in God's name, good King of\n",
            "Your hand: I prithee, peace it in;\n",
            "Without within their leaves \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Window size 25 ⛳"
      ],
      "metadata": {
        "id": "eLnz1aOYF74l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text_util(window_size=25, print_index=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRkGxhtDF2K8",
        "outputId": "0c10032d-56d2-4efc-f276-3aa605bbc40b"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caius Marcius is chief enemy to the people.\n",
            "\n",
            "SICINIUS:\n",
            "Go, masters, look you, sir,\n",
            "I throw a true repune no more.\n",
            "\n",
            "Clown:\n",
            "Ay, by my affairs--but this:\n",
            "Sportuo like an ass which\n",
            "Was for execution but asleep:\n",
            "I never like it not unkind'st!\n",
            "I never luance, and says he\n",
            "Lood's rest that is his looks.\n",
            "\n",
            "BRUTUS:\n",
            "Then calls you writ.\n",
            "\n",
            "LADY PERCY:\n",
            "\n",
            "TYBALT:\n",
            "What, art thou did? my cousins\n",
            "Apon thy woman title to your place.\n",
            "\n",
            "CAPULET:\n",
            "Come, come, my man, myself,\n",
            "I never long'd his father\n",
            "Teeming arms hunged for his deeds,\n",
            "And nigh good metaring hate\n",
            "Money's a fen cozer: he sprails it?\n",
            "\n",
            "ESCALUS:\n",
            "I no more sole agreement\n",
            "To be into and so cross to\n",
            "their names.\n",
            "\n",
            "LEONTES:\n",
            "Thou darest not, take away the life\n",
            "Provingly lies the flandest\n",
            "And make me die the time to move\n",
            "With heavy as an hurd eyes\n",
            "To dwell when Gaudon of my mind\n",
            "With peacery bending thee!\n",
            "\n",
            "ROMEO:\n",
            "But see, or him my mouth,\n",
            "Which I find that thy soldiers, I\n",
            "prothess a herdser here with content\n",
            "That babes receive: what doth he\n",
            "tells us, yourself, or take it\n",
            "ere at the pleasure of by g\n"
          ]
        }
      ]
    }
  ]
}