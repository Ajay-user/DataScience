{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "load data from The 20 newsgroups text dataset, which comprises around 18000 newsgroups posts on 20 topics split in two subsets: one for training and one for testing. For the sake of simplicity and reducing the computational cost, we select a subset of 7 topics and use the training set only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 20 newsgroups training data\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "categories = [\n",
    "    \"alt.atheism\",\n",
    "    \"comp.graphics\",\n",
    "    \"comp.sys.ibm.pc.hardware\",\n",
    "    \"misc.forsale\",\n",
    "    \"rec.autos\",\n",
    "    \"sci.space\",\n",
    "    \"talk.religion.misc\",\n",
    "]\n",
    "\n",
    "print(\"Loading 20 newsgroups training data\")\n",
    "raw_data, _ = fetch_20newsgroups(subset=\"train\", categories=categories, return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents :  3803\n"
     ]
    }
   ],
   "source": [
    "print('Number of documents : ',len(raw_data))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token\n",
    "A token may be a word, part of a word or anything comprised between spaces or symbols in a string. Here we define a function that extracts the tokens using a simple regular expression (regex) that matches Unicode word characters. This includes most characters that can be part of a word in any language, as well as numbers and the underscore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_tokenizer(doc):\n",
    "    tokens = [tok.lower() for tok in re.findall(r\"\\w+\", doc)]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'world']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# usage\n",
    "custom_tokenizer('hello world')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token frequency\n",
    "A function that counts the (frequency of) occurrence of each token in a given document. It returns a frequency dictionary to be used by the vectorizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_frequency(text):\n",
    "    tokens = custom_tokenizer(text)\n",
    "    freq_dict = defaultdict(int)\n",
    "    for tok in tokens:\n",
    "        freq_dict[tok] += 1\n",
    "    return freq_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'sklearn': 1,\n",
       "             'is': 2,\n",
       "             'awesome': 1,\n",
       "             'this': 1,\n",
       "             'a': 1,\n",
       "             'helloworld': 1,\n",
       "             'notebook': 1})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# usage\n",
    "token_freq = token_frequency('SKlearn is awesome. this is a helloworld notebook')\n",
    "token_freq"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words representation\n",
    "**Breaking a text document into word tokens, potentially losing the order information between the words in a sentence is often called a Bag of Words representation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = defaultdict(list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the feature space is not large enough, hashing functions tend to map distinct values to the same hash code (hash collisions). As a result, it is impossible to determine what object generated any particular hash code.\n",
    "\n",
    "So to estimate the number of unique terms in the original dictionary is to count the number of active columns in the encoded feature matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when we use feature hash use this util to count the # of unique terms\n",
    "def non_zero_columns(x):\n",
    "    # np.nonzero(x)\n",
    "    # Return the indices of the elements that are non-zero.\n",
    "    # Returns a tuple of arrays, one for each dimension of a\n",
    "    row, col = np.nonzero(x)\n",
    "\n",
    "    # find distinct cols\n",
    "    distinct = np.unique(col)\n",
    "    return distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_vectorizer(vectorizer, title, is_hash=False, is_raw=False ,is_doc=False):\n",
    "    t0 = time()\n",
    "    if is_hash and is_raw:\n",
    "        # When we set input_type=\"string\" in the FeatureHasher,\n",
    "        #  it vectorize the strings output directly from word tokens\n",
    "        transformed = vectorizer.fit_transform([custom_tokenizer(doc) for doc in raw_data])\n",
    "    elif is_doc:\n",
    "        # CountVectorizer is optimized by reusing a compiled regular expression for the full training set\n",
    "        # instead of creating one per document as done in our naive tokenize function.\n",
    "        # Convert a collection of text documents to a matrix of token counts.\n",
    "        transformed = vectorizer.fit_transform(raw_data)\n",
    "    else:\n",
    "        transformed = vectorizer.fit_transform([token_frequency(doc) for doc in raw_data])\n",
    "\n",
    "        \n",
    "    duration = time() - t0\n",
    "\n",
    "    result_dict['vectorizer'].append(f\"{vectorizer.__class__.__name__} -- {title}\")\n",
    "    result_dict['duration'].append(duration)\n",
    "\n",
    "    terms = len(non_zero_columns(transformed) )if is_hash else len(vectorizer.get_feature_names_out())\n",
    "    print('Number of unique terms :',terms)\n",
    "    print('Time cost :',duration)\n",
    "    return vectorizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dict vectorizer\n",
    "\n",
    "* Transforms lists of feature-value mappings to vectors.\n",
    "* When feature values are strings, this transformer will do a binary one-hot (aka one-of-K) coding:\n",
    "* If a feature value is a sequence or set of strings, this transformer will iterate over the values and will count the occurrences of each string value.\n",
    "* Features that do not occur in a sample (mapping) will have a zero value in the resulting array/matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique terms : 47928\n",
      "Time cost : 1.8799326419830322\n"
     ]
    }
   ],
   "source": [
    "dict_vec = DictVectorizer()\n",
    "\n",
    "dict_vec = test_vectorizer(dict_vec, title='on freq dicts')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual mapping from text token to column index is explicitly stored in the `.vocabulary_ attribute` which is a potentially very large Python dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocab : 47928\n"
     ]
    }
   ],
   "source": [
    "print('Size of vocab :',len(dict_vec.vocabulary_)) # This is a dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping for word \"the\" is  42976\n",
      "Mapping for word \"world\" is  47125\n",
      "Mapping for word \"book\" is  10872\n"
     ]
    }
   ],
   "source": [
    "print('Mapping for word \"the\" is ',dict_vec.vocabulary_['the'])\n",
    "print('Mapping for word \"world\" is ',dict_vec.vocabulary_['world'])\n",
    "print('Mapping for word \"book\" is ',dict_vec.vocabulary_['book'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]]\n",
      "Shape :  (1, 47928)\n"
     ]
    }
   ],
   "source": [
    "text = 'This book is amazing'\n",
    "transformation = dict_vec.transform(token_frequency(text))\n",
    "print(transformation.todense())\n",
    "print('Shape : ',(transformation.todense()).shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FeatureHasher\n",
    "\n",
    "* Dictionaries take up a large amount of storage space and grow in size as the training set grows. \n",
    "* Instead of growing the vectors along with a dictionary, feature hashing builds a vector of pre-defined length by applying a hash function h to the features (e.g., tokens), then using the hash values directly as feature indices and updating the resulting vector at those indices. \n",
    "* When the feature space is not large enough, hashing functions tend to map distinct values to the same hash code (hash collisions). As a result, it is impossible to determine what object generated any particular hash code.\n",
    "\n",
    "* Because of the above it is impossible to recover the original tokens from the feature matrix and the best approach to estimate the number of unique terms in the original dictionary is to count the number of active columns in the encoded feature matrix. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The number of unique tokens when using the FeatureHasher is lower than those obtained using the DictVectorizer. This is due to hash collisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique terms : 46896\n",
      "Time cost : 1.105360984802246\n"
     ]
    }
   ],
   "source": [
    "feat_hash = FeatureHasher()\n",
    "feat_hash = test_vectorizer(feat_hash, title='on freq dicts', is_hash=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default number of features for the FeatureHasher is `2**20`. \n",
    "* Here we set n_features = 2**18 to illustrate hash collisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique terms : 43873\n",
      "Time cost : 1.0374019145965576\n"
     ]
    }
   ],
   "source": [
    "feat_hash = FeatureHasher(n_features=2**18)\n",
    "feat_hash = test_vectorizer(feat_hash, title='on freq dicts', is_hash=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The number of collisions can be reduced by increasing the feature space. Notice that the speed of the vectorizer does not change significantly when setting a large number of features, though it causes larger coefficient dimensions and then requires more memory usage to store them, even if a majority of them is inactive.\n",
    "\n",
    "\n",
    "* We can confirm that the number of unique tokens gets closer to the number of unique terms found by the DictVectorizer when we increase the feature-space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique terms : 47668\n",
      "Time cost : 1.25927734375\n"
     ]
    }
   ],
   "source": [
    "feat_hash = FeatureHasher(n_features=2**22)\n",
    "feat_hash = test_vectorizer(feat_hash, title='on freq dicts', is_hash=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FeatureHasher on raw tokens\n",
    "* one can set `input_type=\"string\"` in the FeatureHasher to vectorize the strings output directly from the customized tokenize function. \n",
    "* This is equivalent to passing a dictionary with an implied frequency of 1 for each feature name.\n",
    "* FeatureHeasher with `input_type=\"string\"` is slightly faster than the variant that works on frequency dict because it does not count repeated tokens: each token is implicitly counted once, even if it was repeated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique terms : 47668\n",
      "Time cost : 1.0963690280914307\n"
     ]
    }
   ],
   "source": [
    "feat_hash = FeatureHasher(n_features=2**22, input_type='string')\n",
    "feat_hash = test_vectorizer(feat_hash, title='on raw tokens', is_hash=True, is_raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DictVectorizer -- on freq dicts',\n",
       " 'FeatureHasher -- on freq dicts',\n",
       " 'FeatureHasher -- on raw tokens']"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_dict['vectorizer']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the results\n",
    "In both cases FeatureHasher is approximately twice as fast as DictVectorizer. This is handy when dealing with large amounts of data, with the downside of losing the invertibility of the transformation, which in turn makes the interpretation of a model a more complex task.\n",
    "\n",
    "The FeatureHeasher with `input_type=\"string\"` is slightly faster than the variant that works on frequency dict because it does not count repeated tokens: each token is implicitly counted once, even if it was repeated. Depending on the downstream machine learning task, it can be a limitation or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,4))\n",
    "plt.barh(result_dict['vectorizer'], result_dict['duration'] );\n",
    "plt.xlabel('Time cost')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./plots/Feat_hash_vs_Dict_vectorizer.png'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special purpose text vectorizers\n",
    "* ### CountVectorizer \n",
    "   * `CountVectorizer` accepts raw data as it internally implements tokenization and occurrence counting.\n",
    "   * The CountVectorizer is more flexible, it accepts various regex patterns through the token_pattern parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique terms : 47885\n",
      "Time cost : 1.2202889919281006\n"
     ]
    }
   ],
   "source": [
    "count_vec = CountVectorizer()\n",
    "count_vec = test_vectorizer(count_vec, title='on raw data', is_doc=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We see that using the CountVectorizer implementation is approximately twice as fast as using the DictVectorizer along with the simple function we defined for mapping the tokens. The reason is that CountVectorizer is optimized by reusing a compiled regular expression for the full training set instead of creating one per document as done in our naive tokenize function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HashingVectorizer\n",
    "Now we make a similar experiment with the HashingVectorizer\n",
    "* HashingVectorizer is equivalent to combining \n",
    "    * the “hashing trick” implemented by the FeatureHasher class\n",
    "    * and the text preprocessing and tokenization of the CountVectorizer.\n",
    "\n",
    "**This strategy has several advantages:**\n",
    "\n",
    "* it is very low memory scalable to large datasets as there is no need to store a vocabulary dictionary in memory.\n",
    "\n",
    "* it is fast to pickle and un-pickle as it holds no state besides the constructor parameters.\n",
    "\n",
    "* it can be used in a streaming (partial fit) or parallel pipeline as there is no state computed during fit.\n",
    "\n",
    "**There are also a couple of cons (vs using a CountVectorizer with an in-memory vocabulary):** \n",
    "\n",
    "* there is no way to compute the inverse transform (from feature indices to string feature names) which can be a problem when trying to introspect which features are most important to a model.\n",
    "\n",
    "* there can be collisions: distinct tokens can be mapped to the same feature index. However in practice this is rarely an issue if n_features is large enough (**e.g. 2 ** 18 for text classification problems**).\n",
    "\n",
    "* no IDF weighting as this would render the transformer stateful.\n",
    "\n",
    "#### We can see that this is the fastest text tokenization strategy so far, assuming that the downstream machine learning task can tolerate a few collisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique terms : 43837\n",
      "Time cost : 0.8495104312896729\n"
     ]
    }
   ],
   "source": [
    "\n",
    "hash_vec = HashingVectorizer(n_features=2**18)\n",
    "\n",
    "hash_vec = test_vectorizer(hash_vec, is_doc=True, is_hash=True, title='on raw data')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfidfVectorizer\n",
    "\n",
    "* Convert a collection of raw documents to a matrix of TF-IDF features.\n",
    "\n",
    "* Equivalent to CountVectorizer followed by TfidfTransformer.\n",
    "\n",
    "In a large text corpus, some words appear with higher frequency `(e.g. “the”, “a”, “is” in English)` and do not carry meaningful information about the actual contents of a document. \n",
    "\n",
    "If we were to feed the word count data directly to a classifier, those very common terms would shadow the frequencies of rarer yet more informative terms. \n",
    "\n",
    "In order to re-weight the count features into floating point values suitable for usage by a classifier it is very common to use the `tf–idf transform` as implemented by the `TfidfTransformer`. \n",
    "\n",
    "`TF` stands for `“term-frequency”` while `“tf–idf”` means `term-frequency times inverse document-frequency`.\n",
    "\n",
    "The `TfidfVectorizer`, which is equivalent to combining the tokenization and occurrence counting of the CountVectorizer along with the normalizing and weighting from a TfidfTransformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique terms : 47885\n",
      "Time cost : 1.2552766799926758\n"
     ]
    }
   ],
   "source": [
    "tf_idf = TfidfVectorizer()\n",
    "tf_idf = test_vectorizer(tf_idf, is_doc=True, title='on raw data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,4))\n",
    "plt.barh(result_dict['vectorizer'], result_dict['duration'] );\n",
    "plt.xlabel('Time cost')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./plots/compare_vectorizers_in_sklearn.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9dd57416d08487125cddc31714a1d5a29ab9aaf930e8420812b05ce6347e3520"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
