{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12bcd974",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6f9cf2",
   "metadata": {},
   "source": [
    "## Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb3f07cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total documents in our dataset 2000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(['welcome', 'to', 'your', 'oh', '-', 'so', 'typical', ...], 'neg'),\n",
       " (['susan', 'granger', \"'\", 's', 'review', 'of', '\"', ...], 'neg'),\n",
       " (['the', 'long', 'kiss', 'goodnight', '(', 'r', ')', ...], 'pos'),\n",
       " (['i', 'heard', 'actor', 'skeet', 'ulrich', ...], 'neg'),\n",
       " (['long', 'ago', ',', 'films', 'were', 'constructed', ...], 'neg')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets create a  dataset using movie reviws data\n",
    "documents = []\n",
    "for category in movie_reviews.categories():\n",
    "    for file in movie_reviews.fileids(category):\n",
    "        review = movie_reviews.words(fileids=file)\n",
    "        documents.append((review,category))\n",
    "        \n",
    "# total documents \n",
    "print('total documents in our dataset',len(documents))\n",
    "\n",
    "# let's shuffle the data\n",
    "random.shuffle(documents)\n",
    "\n",
    "# lets check the first five doc + labels\n",
    "documents[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdcc7ea",
   "metadata": {},
   "source": [
    "## How to create a vocabulary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70a6a64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of vocabulary 1583820\n"
     ]
    }
   ],
   "source": [
    "# this give us all the words in the movie review data\n",
    "vocab = movie_reviews.words()\n",
    "# length of vocab\n",
    "print('Length of vocabulary',len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d357dc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets check the distribution of words\n",
    "distribution = nltk.FreqDist(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57323a67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 77717),\n",
       " ('the', 76529),\n",
       " ('.', 65876),\n",
       " ('a', 38106),\n",
       " ('and', 35576),\n",
       " ('of', 34123),\n",
       " ('to', 31937),\n",
       " (\"'\", 30585),\n",
       " ('is', 25195),\n",
       " ('in', 21822)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what are the most common words -- lets see top 10\n",
    "distribution.most_common()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d210895c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we need to remove the words which are common in natural language\n",
    "# for example \n",
    "# words like 'the', 'a', 'an', 'this', 'that'\n",
    "# Here for our movie review classification these words are not relevent\n",
    "english_stopwords = stopwords.words('english')\n",
    "english_stopwords[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f293e7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# improved vocabulary for movie review classification problem\n",
    "vocab = [word for word in movie_reviews.words() if word not in english_stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00b46702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 77717),\n",
       " ('.', 65876),\n",
       " (\"'\", 30585),\n",
       " ('\"', 17612),\n",
       " ('-', 15595),\n",
       " (')', 11781),\n",
       " ('(', 11664),\n",
       " ('film', 9517),\n",
       " ('one', 5852),\n",
       " ('movie', 5771)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what are the most common words in our improved vocab -- lets see top 10\n",
    "# lets check the distribution of words\n",
    "distribution = nltk.FreqDist(vocab)\n",
    "distribution.most_common()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1598d1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of vocabulary 955610\n"
     ]
    }
   ],
   "source": [
    "# length of vocab\n",
    "print('Length of vocabulary',len(vocab))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6849cf",
   "metadata": {},
   "source": [
    "## Creating Feature vector\n",
    "\n",
    "how can we take a review and turn it into a feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93c50268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['welcome', 'to', 'your', 'oh', '-', 'so', 'typical', ...]\n",
      "1180 words in this movie review\n",
      "sentiment :  neg\n"
     ]
    }
   ],
   "source": [
    "# a moview review\n",
    "print(documents[0][0])\n",
    "# length of the review\n",
    "print(len(documents[0][0]),'words in this movie review')\n",
    "print('sentiment : ',documents[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64d5c000",
   "metadata": {},
   "outputs": [],
   "source": [
    "### lets take top 500 frequent words from the vocab\n",
    "\n",
    "frequent_500 = distribution.most_common()[:500]\n",
    "\n",
    "# for each word in this list\n",
    "# if that word is in the movie review we put a '1' or 'True'\n",
    "# else we put '0' or 'False'\n",
    "# the result will be a multi-hot-vector\n",
    "frequent_500 = [tup[0] for tup in frequent_500]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f287834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2.12 s\n",
      "Wall time: 2.11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Multi hot vectors\n",
    "feature_vectors = []\n",
    "\n",
    "for review, sentiment in documents:\n",
    "    multi_hot_vector = {}\n",
    "    # we are converting reviews into a dict\n",
    "    # words in review are keys \n",
    "    # and for every key we set True as value    \n",
    "    review_lookup = {word:True for word in review}\n",
    "    \n",
    "    for word in frequent_500:\n",
    "        try:\n",
    "            if review_lookup[word]:\n",
    "                multi_hot_vector[word] = True\n",
    "        except:\n",
    "            multi_hot_vector[word] = False\n",
    "            \n",
    "    feature_vectors.append(tuple([multi_hot_vector, sentiment]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "adff5ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test driven development\n",
    "assert len(documents) == len(feature_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85613ed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True, True, True, True, True, True, True, False, True, True]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now lets take a look at the first vector\n",
    "\n",
    "list(feature_vectors[0][0].values())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f928d580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neg'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentiment\n",
    "feature_vectors[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05cc54e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is a very naive approach we will improve our features in later notebooks\n",
    "## This is just an intro to feature building\n",
    "## These features may or may not be good, because we are including most frequent 500 words as components\n",
    "## if these 500 words are enough to represent the sentiments then we are in luck \n",
    "\n",
    "## For creating our feature vectors\n",
    "## we have to convert reviews to lookup dict\n",
    "## otherwise there will be a time complexity issue\n",
    "## for example suppose our code is\n",
    "\n",
    "# '''\n",
    "\n",
    "# for review, sentiment in documents:\n",
    "#     multi_hot_vector = []\n",
    "    \n",
    "#     for word in frequent_500:\n",
    "    \n",
    "#         ###### to create each component we have to search the entire length of review ######\n",
    "\n",
    "#         if word in review:                            <<<< TIME COMPLEXITY >>>>\n",
    "#             multi_hot_vector.append(True)\n",
    "#         else:\n",
    "#             multi_hot_vector.append(False)\n",
    "            \n",
    "#     feature_vectors.append((multi_hot_vector, sentiment))  \n",
    "    \n",
    "# ''' \n",
    "\n",
    "# there are 2000 reviews\n",
    "# for each \n",
    "# we have to do ~ 500 * len(review) to create the multi-hot-vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e97a86ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many True/1 in feature vector for review 1\n",
    "sum(feature_vectors[0][0].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dc86039a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(feature_vectors[1][0].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d662e089",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(feature_vectors[2][0].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "927329a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(feature_vectors[3][0].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "169e5ecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(feature_vectors[4][0].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9a120f0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# length of feature vector\n",
    "len(feature_vectors[0][0].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e203560f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826b97f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
