{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a883f675",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "import pickle\n",
    "import pathlib\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import movie_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2669a0",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f141187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total documents in our dataset 2000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(['it', \"'\", 's', 'now', 'the', 'anniversary', 'of', ...], 'neg'),\n",
       " (['my', 'summer', 'was', 'recently', 'saved', 'by', ...], 'pos'),\n",
       " (['just', 'look', 'back', 'two', 'years', 'ago', 'at', ...], 'pos'),\n",
       " (['it', \"'\", 's', 'difficult', 'to', 'expect', 'much', ...], 'neg'),\n",
       " (['roberto', 'benigni', 'is', 'a', 'clown', 'in', 'the', ...], 'pos')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets create a  dataset using movie reviws data\n",
    "documents = []\n",
    "for category in movie_reviews.categories():\n",
    "    for file in movie_reviews.fileids(category):\n",
    "        review = movie_reviews.words(fileids=file)\n",
    "        documents.append((review,category))\n",
    "        \n",
    "# total documents \n",
    "print('total documents in our dataset',len(documents))\n",
    "\n",
    "# let's shuffle the data\n",
    "random.shuffle(documents)\n",
    "\n",
    "# lets check the first five doc + labels\n",
    "documents[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218eee72",
   "metadata": {},
   "source": [
    "## Vocabulary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b4d79be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of vocabulary 1583820\n",
      "STOP WORDS ['i', 'me', 'my', 'myself', 'we']\n",
      "Most common words in the dataset [(',', 77717), ('.', 65876), (\"'\", 30585), ('\"', 17612), ('-', 15595), (')', 11781), ('(', 11664), ('film', 9517), ('one', 5852), ('movie', 5771)]\n",
      "Length of vocabulary after removing stop words 955610\n"
     ]
    }
   ],
   "source": [
    "# this give us all the words in the movie review data\n",
    "vocab = movie_reviews.words()\n",
    "# length of vocab\n",
    "print('Length of vocabulary',len(vocab))\n",
    "\n",
    "# stopwords\n",
    "english_stopwords = stopwords.words('english')\n",
    "print('STOP WORDS',english_stopwords[:5])\n",
    "\n",
    "# improved vocabulary for movie review classification problem\n",
    "vocab = [word for word in movie_reviews.words() if word not in english_stopwords]\n",
    "\n",
    "\n",
    "# lets check the distribution of words\n",
    "distribution = nltk.FreqDist(vocab)\n",
    "print('Most common words in the dataset',distribution.most_common()[:10])\n",
    "\n",
    "# length of vocab\n",
    "print('Length of vocabulary after removing stop words',len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80c79d7",
   "metadata": {},
   "source": [
    "## Creating Feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90217dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### lets take top 500 frequent words from the vocab\n",
    "\n",
    "frequent_500 = distribution.most_common()[:500]\n",
    "\n",
    "# for each word in this list\n",
    "# if that word is in the movie review we put a '1' or 'True'\n",
    "# else we put '0' or 'False'\n",
    "# the result will be a multi-hot-vector\n",
    "frequent_500 = [tup[0] for tup in frequent_500]\n",
    "\n",
    "\n",
    "\n",
    "## Multi hot vectors\n",
    "feature_vectors = []\n",
    "\n",
    "for review, sentiment in documents:\n",
    "    multi_hot_vector = {}\n",
    "    # we are converting reviews into a dict\n",
    "    # words in review are keys \n",
    "    # and for every key we set True as value    \n",
    "    review_lookup = {word:True for word in review}\n",
    "    \n",
    "    for word in frequent_500:\n",
    "        try:\n",
    "            if review_lookup[word]:\n",
    "                multi_hot_vector[word] = True\n",
    "        except:\n",
    "            multi_hot_vector[word] = False\n",
    "            \n",
    "    feature_vectors.append(tuple([multi_hot_vector, sentiment]))  \n",
    "    \n",
    "    \n",
    "# Test driven development\n",
    "assert len(documents) == len(feature_vectors)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22544964",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1472ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = feature_vectors[:1900]\n",
    "test_data = feature_vectors[1900:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc1bcc8",
   "metadata": {},
   "source": [
    "## Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75794a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.78\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "bayes = nltk.NaiveBayesClassifier.train(train_data)\n",
    "# score on test\n",
    "accuracy = nltk.classify.accuracy(bayes, test_data)\n",
    "print('Accuracy : ',accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55edb581",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ee73b7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('saved_models')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = pathlib.Path('./saved_models/')\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e58d48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path/'bayes_classifier.pickle','wb') as f:\n",
    "    pickle.dump(bayes, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29683bee",
   "metadata": {},
   "source": [
    "## load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7e9ef74",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path/'bayes_classifier.pickle','rb') as f:\n",
    "    reloaded_bayes = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a9c3c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.78\n"
     ]
    }
   ],
   "source": [
    "accuracy = nltk.classify.accuracy(reloaded_bayes, test_data)\n",
    "print('Accuracy : ',accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "09dbf1e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('worst', True),\n",
       " ('stupid', True),\n",
       " ('boring', True),\n",
       " ('perfect', True),\n",
       " ('supposed', True),\n",
       " ('worse', True),\n",
       " ('none', True),\n",
       " ('others', True),\n",
       " ('oscar', True),\n",
       " ('performances', True)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print 10 most useful feature components  ie - words\n",
    "\n",
    "reloaded_bayes.most_informative_features(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0721dd",
   "metadata": {},
   "source": [
    "## Let's use the model for classfying review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8eae22b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model prediction pos\n",
      "True label pos\n"
     ]
    }
   ],
   "source": [
    "review = test_data[0][0]\n",
    "true_label = test_data[0][1]\n",
    "\n",
    "prediction = reloaded_bayes.classify(review)\n",
    "print(\"Model prediction\",prediction)\n",
    "print(\"True label\",true_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c17dea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
