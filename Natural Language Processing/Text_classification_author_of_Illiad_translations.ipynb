{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text classification author of Illiad translations.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPhMHjj97f96o6rfOGsaWA4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ajay-user/DataScience/blob/master/Natural%20Language%20Processing/Text_classification_author_of_Illiad_translations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNRJjwLH2M7S"
      },
      "source": [
        "## Predict the author of Illiad translations\n",
        "The following provides an example of using `tf.data.TextLineDataset` to load examples from text files, and `tf.text` to preprocess the data. In this example, you will use three different English translations of the same work, Homer's Illiad, and train a model to identify the translator given a single line of text.\n",
        "\n",
        "Download and explore the dataset\n",
        "\n",
        "The texts of the three translations are by:\n",
        "\n",
        "* William Cowper — text\n",
        "\n",
        "* Edward, Earl of Derby — text\n",
        "\n",
        "* Samuel Butler — text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-8MRpaB2LJ5",
        "outputId": "c63472c4-e693-495c-893e-3137ff834bd0"
      },
      "source": [
        "pip install tensorflow-text-nightly"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-text-nightly\n",
            "  Downloading tensorflow_text_nightly-2.7.0.dev20210825-cp37-cp37m-manylinux1_x86_64.whl (4.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.3 MB 8.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text-nightly) (0.12.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-hub>=0.8.0->tensorflow-text-nightly) (3.17.3)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-hub>=0.8.0->tensorflow-text-nightly) (1.19.5)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorflow-hub>=0.8.0->tensorflow-text-nightly) (1.15.0)\n",
            "Installing collected packages: tensorflow-text-nightly\n",
            "Successfully installed tensorflow-text-nightly-2.7.0.dev20210825\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tyNEziIYAGCS",
        "outputId": "9112bfe0-96d3-4a25-d96e-a5948b924735"
      },
      "source": [
        "pip install tensorflow-text"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-text\n",
            "  Downloading tensorflow_text-2.6.0-cp37-cp37m-manylinux1_x86_64.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 7.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text) (0.12.0)\n",
            "Requirement already satisfied: tensorflow<2.7,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text) (2.6.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text) (3.7.4.3)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text) (1.12.1)\n",
            "Requirement already satisfied: tensorflow-estimator~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text) (2.6.0)\n",
            "Requirement already satisfied: clang~=5.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text) (5.0)\n",
            "Requirement already satisfied: keras~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text) (2.6.0)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text) (2.6.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text) (1.1.0)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text) (0.4.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text) (3.3.0)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text) (1.19.5)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text) (1.12)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text) (0.37.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text) (1.1.2)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text) (1.15.0)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text) (3.1.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text) (0.2.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text) (1.6.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.37.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text) (1.39.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text) (3.17.3)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text) (0.12.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow<2.7,>=2.6.0->tensorflow-text) (1.5.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text) (0.4.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text) (3.3.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text) (2.23.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text) (1.34.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text) (1.8.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text) (57.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text) (4.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text) (4.6.4)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text) (3.5.0)\n",
            "Installing collected packages: tensorflow-text\n",
            "Successfully installed tensorflow-text-2.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLmOtaCA4OXs"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3NXpES23JHY"
      },
      "source": [
        "import tensorflow as tf\n",
        "import pathlib\n",
        "import tensorflow_text as tf_text\n",
        "import collections"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdzPNNC24R8X"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IE_FoJM3bBC",
        "outputId": "51e71dac-29dd-4b86-a208-9ab77aa74561"
      },
      "source": [
        "DIRECTORY_URL = 'https://storage.googleapis.com/download.tensorflow.org/data/illiad/'\n",
        "FILE_NAMES = ['cowper.txt', 'derby.txt', 'butler.txt']\n",
        "\n",
        "text_dir = ''\n",
        "\n",
        "for author in FILE_NAMES:\n",
        "  text_dir = tf.keras.utils.get_file(fname=author, origin=DIRECTORY_URL+author)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/illiad/cowper.txt\n",
            "819200/815980 [==============================] - 0s 0us/step\n",
            "827392/815980 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/illiad/derby.txt\n",
            "811008/809730 [==============================] - 0s 0us/step\n",
            "819200/809730 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/illiad/butler.txt\n",
            "811008/807992 [==============================] - 0s 0us/step\n",
            "819200/807992 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3warbKc4BbW",
        "outputId": "7936914c-e84f-4ce0-cf89-228cc4c02f35"
      },
      "source": [
        "parent_dir =  pathlib.Path(text_dir).parent\n",
        "\n",
        "print('Parent directory ',parent_dir)\n",
        "for dir in parent_dir.iterdir():\n",
        "  print('File :',dir)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parent directory  /root/.keras/datasets\n",
            "File : /root/.keras/datasets/derby.txt\n",
            "File : /root/.keras/datasets/cowper.txt\n",
            "File : /root/.keras/datasets/butler.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0FvYZL_-Mpb"
      },
      "source": [
        "## Model constant"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBwAL2aL-Lk1"
      },
      "source": [
        "BUFFER_SIZE = 50000\n",
        "BATCH_SIZE = 64\n",
        "VALIDATION_SIZE = 5000\n",
        "VOCAB_SIZE = 10000\n",
        "MAX_SEQUENCE_LENGTH = 250"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6y465j8M5JNi"
      },
      "source": [
        "## Load the dataset\n",
        "\n",
        " `TextLineDataset` is designed to create a `tf.data.Dataset` from a text file in which each example is a line of text from the original file, whereas `text_dataset_from_directory` treats all contents of a file as a single example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejdvrY_d4eUW"
      },
      "source": [
        "def labeler(sample, label):\n",
        "  return (sample, tf.cast(label, tf.int64))\n",
        "\n",
        "\n",
        "lines_set = []\n",
        "for label, fname in enumerate(FILE_NAMES):\n",
        "  lines_ds = tf.data.TextLineDataset(str(parent_dir/fname))\n",
        "  labeled_ds = lines_ds.map(lambda sample:labeler(sample, label))\n",
        "  lines_set.append(labeled_ds)\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9uhjSIz6PKs",
        "outputId": "3a3fbd35-ed8a-4c02-8c8c-cf2e53804243"
      },
      "source": [
        "print('Length of lines set ', len(lines_set))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of lines set  3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lbpm5YLK6oNZ"
      },
      "source": [
        "text_lines_ds = lines_set[0]\n",
        "\n",
        "# concatenation\n",
        "for ds in lines_set[1:]:\n",
        "  text_lines_ds = text_lines_ds.concatenate(ds)\n",
        "\n",
        "# shuffle the dataset\n",
        "text_lines_ds = text_lines_ds.shuffle(buffer_size=BUFFER_SIZE, seed=42, reshuffle_each_iteration=False, )"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGipIviZ9LVR",
        "outputId": "b89dad77-bff3-4e9e-cb63-ce20967e6b5e"
      },
      "source": [
        "for line, label in text_lines_ds.take(5):\n",
        "  print('Line : ',line.numpy())\n",
        "  print('Label : ',label.numpy())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Line :  b'taught to use the bow.'\n",
            "Label :  2\n",
            "Line :  b\"This said, he sat; and Atreus' godlike son,\"\n",
            "Label :  1\n",
            "Line :  b'Is gone to Chrysa, and with her we send'\n",
            "Label :  0\n",
            "Line :  b\"He cut the boar's throat as he spoke, whereon Talthybius whirled it\"\n",
            "Label :  2\n",
            "Line :  b\"Redden'd the east, then, thronging forth, all Troy\"\n",
            "Label :  0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iA0TQh_-_qBc"
      },
      "source": [
        "## Prepare the dataset for training\n",
        "\n",
        "Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvxODMrc9aAr"
      },
      "source": [
        "#tokenizer\n",
        "tokenizer = tf_text.UnicodeScriptTokenizer()\n",
        "\n",
        "# utility for standardizing and tokenizing the text\n",
        "def tokenize(text):\n",
        "  lowercase = tf_text.case_fold_utf8(text)\n",
        "  return tokenizer.tokenize(lowercase)\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2zYpomeAUk-",
        "outputId": "23db8ff7-b096-4df6-e9ad-44c84c215cb7"
      },
      "source": [
        "# eg: tokenization\n",
        "(line, label) = next(iter(text_lines_ds))\n",
        "\n",
        "print('Line : ',line.numpy())\n",
        "print('Tokenization : ',tokenize(line).numpy())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Line :  b'taught to use the bow.'\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:206: batch_gather (from tensorflow.python.ops.array_ops) is deprecated and will be removed after 2017-10-25.\n",
            "Instructions for updating:\n",
            "`tf.batch_gather` is deprecated, please use `tf.gather` with `batch_dims=-1` instead.\n",
            "Tokenization :  [b'taught' b'to' b'use' b'the' b'bow' b'.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtzeU0ZCMVnT"
      },
      "source": [
        "# create a tokenized dataset\n",
        "tokenized_ds = text_lines_ds.map(lambda text, label: tokenize(text))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCI3P1koDUk6"
      },
      "source": [
        "Building Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxJg-SLVCO_R"
      },
      "source": [
        "# create a default dict for storing vocab\n",
        "vocab_dict = collections.defaultdict(lambda : 0)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_RYst1ICbOh"
      },
      "source": [
        "# iterate through the tokenized dataset and create the vocabulary\n",
        "for tokens in tokenized_ds.as_numpy_iterator():\n",
        "  for toks in tokens:\n",
        "    vocab_dict[toks] += 1"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jq8UAInoNmko",
        "outputId": "6c7c634d-6378-46ef-e1a0-03d49f7b6a72"
      },
      "source": [
        "# sorting\n",
        "vocab = sorted(vocab_dict.items(), key=lambda tup: tup[1], reverse=True)\n",
        "\n",
        "print('Length of vocab', len(vocab))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of vocab 14262\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLTWI4JxOank",
        "outputId": "f5ce1d25-55d1-4d2b-f85f-2af5b8656d55"
      },
      "source": [
        "# we only need vocabs not the counts\n",
        "vocab = [token for token, count in vocab]\n",
        "\n",
        "# keep the top VOCAB_SIZE only\n",
        "vocab = vocab[:VOCAB_SIZE]\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "print('Lenght of vocab', vocab_size)\n",
        "print('First five vocab', vocab[:5])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Lenght of vocab 10000\n",
            "First five vocab [b',', b'the', b'and', b\"'\", b'of']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Die3yYZ1VFEV"
      },
      "source": [
        "Vocab look-up table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AirSC3RWQTIU"
      },
      "source": [
        "# vocab look-up\n",
        "\n",
        "keys = vocab\n",
        " # reserve 0 for padding, 1 for OOV\n",
        "values = range(2,vocab_size+2)\n",
        "\n",
        "# key-value intializer\n",
        "init = tf.lookup.KeyValueTensorInitializer(keys, values, key_dtype=tf.string, value_dtype=tf.int64)\n",
        "\n",
        "# look-up table\n",
        "num_oov = 1\n",
        "vocab_table = tf.lookup.StaticVocabularyTable(init, num_oov)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYr7l25tVNhz"
      },
      "source": [
        "Standardize, Tokenize, Vectorize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuFdEKmpU4i3"
      },
      "source": [
        "def preprocess_text(text, label):\n",
        "  standardize = tf_text.case_fold_utf8(text)\n",
        "  tokenize = tokenizer.tokenize(standardize)\n",
        "  vectorize = vocab_table.lookup(tokenize)\n",
        "  return vectorize, label"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "McLRhC7XVv6d",
        "outputId": "f423895b-2f40-4047-cb41-ad940df71554"
      },
      "source": [
        "(text, label) = next(iter(text_lines_ds))\n",
        "print('Line', text.numpy())\n",
        "preprocessed_text, preprocessed_label = preprocess_text(text, label)\n",
        "print('Preprocessed output', preprocessed_text.numpy())"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Line b'taught to use the bow.'\n",
            "Preprocessed output [1595    8 1596    3  310    7]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMLwFQGmV4q8"
      },
      "source": [
        "# standardize, tokenize and vectorize the dataset using the tokenizer and lookup table\n",
        "encoded_ds = text_lines_ds.map(preprocess_text)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSzNaxMC1_zO"
      },
      "source": [
        "## Train Test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvtnkZAf1pdQ"
      },
      "source": [
        "train_ds = encoded_ds.skip(VALIDATION_SIZE)\n",
        "val_ds = encoded_ds.take(VALIDATION_SIZE)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16qp3XV62Qil"
      },
      "source": [
        "Shuffling , Padding and Batching"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2JzAwTk2OrV"
      },
      "source": [
        "train_ds = train_ds.shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE)\n",
        "val_ds = val_ds.padded_batch(BATCH_SIZE)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIby_Kj39LTx",
        "outputId": "2e9c6cd4-7752-445a-f5d7-d2e89a9958de"
      },
      "source": [
        "text_batch, label_batch = next(iter(train_ds))\n",
        "\n",
        "print('Shape of text batch', text_batch.shape)\n",
        "print('Shape of label batch', label_batch.shape)\n",
        "print('Sample text ',text_batch[0].numpy())\n",
        "print('Sample label ',label_batch[0].numpy())"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of text batch (64, 18)\n",
            "Shape of label batch (64,)\n",
            "Sample text  [ 20  47 387  22  32 415 122   2 816 143   0   0   0   0   0   0   0   0]\n",
            "Sample label  0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqDbWCepA4bn"
      },
      "source": [
        "## Configure for performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mjx3xh_-9fvn"
      },
      "source": [
        "def config_for_performance(ds):\n",
        "  ds = ds.cache()\n",
        "  ds = ds.prefetch(tf.data.AUTOTUNE)\n",
        "  return ds"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeUE60t7BMU-"
      },
      "source": [
        "train_ds = config_for_performance(train_ds)\n",
        "val_ds = config_for_performance(val_ds)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7HrfLTeBUUU"
      },
      "source": [
        "## Model Building"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAwkh3iNBSql",
        "outputId": "51fd9e61-fbe4-4437-e7c0-49c2b5a864fc"
      },
      "source": [
        "#  1D ConvNet\n",
        "model_1 = tf.keras.Sequential([\n",
        "                               tf.keras.layers.Embedding(input_dim=vocab_size+2, output_dim=64, mask_zero=True),\n",
        "                               tf.keras.layers.Conv1D(filters=64, kernel_size=5, padding='valid', activation='relu', strides=2),\n",
        "                               tf.keras.layers.GlobalAveragePooling1D(),\n",
        "                               tf.keras.layers.Dense(3)                              \n",
        "])\n",
        "# compile the model\n",
        "model_1.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
        "# train the model\n",
        "model_1_history = model_1.fit(train_ds, validation_data=val_ds, epochs=3)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "697/697 [==============================] - 42s 20ms/step - loss: 0.5126 - accuracy: 0.7785 - val_loss: 0.4038 - val_accuracy: 0.8362\n",
            "Epoch 2/3\n",
            "697/697 [==============================] - 9s 13ms/step - loss: 0.3214 - accuracy: 0.8704 - val_loss: 0.3864 - val_accuracy: 0.8392\n",
            "Epoch 3/3\n",
            "697/697 [==============================] - 9s 13ms/step - loss: 0.2527 - accuracy: 0.9015 - val_loss: 0.4007 - val_accuracy: 0.8354\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7IsxNzZCfdQ",
        "outputId": "e7b9c1ce-931f-45b0-e9a2-0297918ad8d0"
      },
      "source": [
        "loss, accuracy = model_1.evaluate(val_ds)\n",
        "print('Model loss', loss)\n",
        "print('Model accuracy', accuracy)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "79/79 [==============================] - 1s 3ms/step - loss: 0.4007 - accuracy: 0.8354\n",
            "Model loss 0.4006834030151367\n",
            "Model accuracy 0.8353999853134155\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dO6kdFIOI6Cw",
        "outputId": "daba28fe-cc1a-4f70-df0b-c8628c5861b7"
      },
      "source": [
        "for layer in model_1.layers:\n",
        "  print('layer ',layer, 'supports masking', layer.supports_masking)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "layer  <keras.layers.embeddings.Embedding object at 0x7fd8ed738e50> supports masking True\n",
            "layer  <keras.layers.convolutional.Conv1D object at 0x7fd8ed6239d0> supports masking False\n",
            "layer  <keras.layers.pooling.GlobalAveragePooling1D object at 0x7fd8ed731ad0> supports masking True\n",
            "layer  <keras.layers.core.Dense object at 0x7fd8ef2b0a50> supports masking True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yITw773kKFBB"
      },
      "source": [
        "## Export model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1h9uZNS8JOPs"
      },
      "source": [
        "preprocessing_layer = tf.keras.layers.TextVectorization(max_tokens=vocab_size+2,\n",
        "                                                        standardize=tf_text.case_fold_utf8,\n",
        "                                                        split=tokenizer.tokenize,\n",
        "                                                        output_mode='int',\n",
        "                                                        output_sequence_length=MAX_SEQUENCE_LENGTH)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-lB7zNRJSSv"
      },
      "source": [
        "# setting vocabulary\n",
        "preprocessing_layer.set_vocabulary(vocab)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kt83-Az8LN3Q"
      },
      "source": [
        "# export model\n",
        "export_model = tf.keras.Sequential([preprocessing_layer, model_1, tf.keras.layers.Activation('sigmoid')])\n",
        "\n",
        "# compile the model\n",
        "export_model.compile(optimizer='adam',\n",
        "                     loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "                     metrics=['accuracy'])"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUqI7sgqLTy7"
      },
      "source": [
        "# dataset of raw strings\n",
        "test_ds = text_lines_ds.take(VALIDATION_SIZE).batch(BATCH_SIZE)"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jITAZNgBO50j"
      },
      "source": [
        "# congif for performance\n",
        "test_ds = config_for_performance(test_ds)"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xS-dvEhqNEqf",
        "outputId": "50a9576b-2ac9-49d9-eef9-8e694a2a4586"
      },
      "source": [
        "loss, accuracy = export_model.evaluate(test_ds)\n",
        "print('Model loss', loss)\n",
        "print('Model accuracy', accuracy)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "79/79 [==============================] - 3s 27ms/step - loss: 0.7100 - accuracy: 0.7266\n",
            "Model loss 0.7100275158882141\n",
            "Model accuracy 0.7265999913215637\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptWFMiArPyXE"
      },
      "source": [
        "## Inference on New data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1st6pZCOTAF"
      },
      "source": [
        "inputs = [\n",
        "    \"Join'd to th' Ionians with their flowing robes,\",  # Label: 1\n",
        "    \"the allies, and his armour flashed about him so that he seemed to all\",  # Label: 2\n",
        "    \"And with loud clangor of his arms he fell.\",  # Label: 0\n",
        "]"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjT9qPOPRpls"
      },
      "source": [
        "# making predictions\n",
        "predictions = export_model.predict(inputs)"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctukmgLvRtyi",
        "outputId": "8db4027e-d43f-4dcb-ea99-5f3d1397ea4b"
      },
      "source": [
        "for text, pred in zip(inputs, predictions):\n",
        "  print('Text : ', text)\n",
        "  print('Predicted label : ', tf.argmax(pred).numpy())"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text :  Join'd to th' Ionians with their flowing robes,\n",
            "Predicted label :  1\n",
            "Text :  the allies, and his armour flashed about him so that he seemed to all\n",
            "Predicted label :  2\n",
            "Text :  And with loud clangor of his arms he fell.\n",
            "Predicted label :  0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50yufI2kRzFr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}