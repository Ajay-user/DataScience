{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text generation with an RNN 101 .ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMg/aGdIlooRKzvJ8+lT0XG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ajay-user/ML-DL-RL-repo/blob/master/Natural%20Language%20Processing/Text_generation_with_an_RNN_101_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given a sequence of characters from this data (\"Shakespear\"), train a model to predict the next character in the sequence\n",
        "\n",
        "\n",
        "This notebook is created using tensorflow resources. For more information please use this link [TENSORFLOW](https://www.tensorflow.org/text/tutorials/text_generation#train_the_model)"
      ],
      "metadata": {
        "id": "OuZeJY4pyBA1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lNMtp9YYx3eN"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the data"
      ],
      "metadata": {
        "id": "k8Xh04bTyvJN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt'\n",
        "data_dir = tf.keras.utils.get_file(origin=url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxDZxz7byny6",
        "outputId": "e42deff0-fe18-4cf6-a854-c3528ee5d49c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n",
            "1130496/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read few lines of data"
      ],
      "metadata": {
        "id": "KqxDhkfCzFYz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = open(data_dir,mode='rb').read().decode(encoding='utf-8')\n",
        "\n",
        "# total number of unique characters in the text file\n",
        "vocab = set(text)\n",
        "print(f'lenght of text file:{len(text)}')\n",
        "print('total number of unique characters in the text file :',len(vocab))\n",
        "\n",
        "\n",
        "print('\\nLets read first 250 lines from the file \\n','-'*100)\n",
        "print(text[:250])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_lm1IM8y9_4",
        "outputId": "a11dc8db-4c94-4d67-a426-6bfc87062a53"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lenght of text file:1115394\n",
            "total number of unique characters in the text file : 65\n",
            "\n",
            "Lets read first 250 lines from the file \n",
            " ----------------------------------------------------------------------------------------------------\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utilities for text processing"
      ],
      "metadata": {
        "id": "PJwFEzyk0t_D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(vocabulary=list(vocab))\n",
        "chars_from_ids = tf.keras.layers.StringLookup(vocabulary=ids_from_chars.get_vocabulary(), invert=True)\n",
        "text_from_ids = lambda ids: tf.strings.reduce_join(chars_from_ids(ids), axis=-1)\n",
        "split_input_target = lambda seq: (seq[:-1],seq[1:])"
      ],
      "metadata": {
        "id": "iPuEBtJ3zCq-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create training examples and targets"
      ],
      "metadata": {
        "id": "_S2OXHgF2bLb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_chars = tf.strings.unicode_split(text,'UTF-8')\n",
        "all_ids = ids_from_chars(all_chars)\n",
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "metadata": {
        "id": "o0wT_P0a2eep"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for id in ids_dataset.take(5):\n",
        "  print(f'id:{id.numpy()}, char:{chars_from_ids(id.numpy())}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_q0eXXW44YwK",
        "outputId": "10fb0a2e-9a50-4700-f54e-6ca9e1990132"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id:21, char:b'F'\n",
            "id:54, char:b'i'\n",
            "id:44, char:b'r'\n",
            "id:30, char:b's'\n",
            "id:26, char:b't'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating text sequences "
      ],
      "metadata": {
        "id": "VObhNadM5PPB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.ops.array_ops import sequence_mask\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)"
      ],
      "metadata": {
        "id": "uFIDZH_J4f6D"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets check the sequences we create\n",
        "for seq in sequences.take(1):\n",
        "  print('IDs','-'*100,'\\n',seq.numpy())\n",
        "  print('Characters','-'*100,'\\n',chars_from_ids(seq.numpy()).numpy())\n",
        "  print('String','-'*100,'\\n',tf.strings.join(chars_from_ids(seq.numpy())).numpy().decode('utf-8'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rw6Arxfu5t0B",
        "outputId": "a850a234-4e8a-4aaf-e3b9-fdf48043d189"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IDs ---------------------------------------------------------------------------------------------------- \n",
            " [21 54 44 30 26 43 12 54 26 54 17 52 23 33 58 60 52 63 31 44 52 43 51 52\n",
            " 43 24 44 31 53 52 52 10 43 55 23 49 43 63 47 44 26 20 52 44  7 43 20 52\n",
            " 55 44 43 37 52 43 30 24 52 55 57  4 58 58 46 45 45 33 58 14 24 52 55 57\n",
            "  7 43 30 24 52 55 57  4 58 58 21 54 44 30 26 43 12 54 26 54 17 52 23 33\n",
            " 58 13 31 47 43]\n",
            "Characters ---------------------------------------------------------------------------------------------------- \n",
            " [b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' ']\n",
            "String ---------------------------------------------------------------------------------------------------- \n",
            " First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We have a sequence of 100 characters \n",
        "# now we've to create inputs and labels \n",
        "split_input_target(list(\"Tensorflow\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1JCJxhz7_Ln",
        "outputId": "d355634e-cbad-4546-a49c-db6733015e44"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = sequences.map(split_input_target, num_parallel_calls=tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "iEAMVOYX8zEX"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_seq, target_seq in dataset.take(1):\n",
        "  print('Input sequence\\n',text_from_ids(input_seq))\n",
        "  print('Output sequence\\n',text_from_ids(target_seq))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QaHRMv6J9wS0",
        "outputId": "bb6de91b-c597-435a-d402-320bfa64f80a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input sequence\n",
            " tf.Tensor(b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou', shape=(), dtype=string)\n",
            "Output sequence\n",
            " tf.Tensor(b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou ', shape=(), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create training batches\n",
        "shuffle the data and pack it into batches."
      ],
      "metadata": {
        "id": "_3xUg5A_-vjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. \n",
        "# Instead, it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000"
      ],
      "metadata": {
        "id": "SLMLb48X-H5u"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.shuffle(buffer_size=BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "I1eaau6w_BaR"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Building"
      ],
      "metadata": {
        "id": "l27mZJ1N_nfs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "metadata": {
        "id": "JlACl-4H_eyM"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextGen(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_size, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embeddings = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_size)\n",
        "    self.gru = tf.keras.layers.GRU(units=rnn_units, return_sequences=True, return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(units=vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    # Embedding \n",
        "    x = self.embeddings(inputs, training=training)\n",
        "\n",
        "    # Recurrent Network\n",
        "    if states == None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "\n",
        "    # classifier\n",
        "    logits = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return logits, states\n",
        "    else:\n",
        "      return logits\n"
      ],
      "metadata": {
        "id": "c00FmoOf_u9B"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the model\n",
        "# Be sure the vocabulary size matches the `StringLookup` layers\n",
        "\n",
        "my_model = TextGen(vocab_size=ids_from_chars.vocabulary_size(), embedding_size=embedding_dim, rnn_units=rnn_units)"
      ],
      "metadata": {
        "id": "Eej-Px6wEXXi"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets see our model in action\n",
        "\n",
        "# Input to the model is sequences of length 100\n",
        "# model takes the input sequence and outputs the logits \n",
        "# output shape is [ batch, sequence length , vocabulary lenght ]\n",
        "for seq_in, seq_out in dataset.take(1):\n",
        "  model_out = my_model(seq_in)"
      ],
      "metadata": {
        "id": "vlA3L_3HFGeg"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Shape of input passed into the model',seq_in.shape)\n",
        "print('Shape of output given by the model',model_out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4TxBLlsHQrD",
        "outputId": "5cdab9ae-1075-4101-dca5-f9a969a2ac24"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of input passed into the model (64, 100)\n",
            "Shape of output given by the model (64, 100, 66)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DGkKzUAHsgI",
        "outputId": "c24acef7-2294-4170-e49a-e3b84d12e97d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"text_gen\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  16896     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,022,850\n",
            "Trainable params: 4,022,850\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get actual predictions from the model you need to sample from the output distribution, to get actual character indices. This distribution is defined by the logits over the character vocabulary."
      ],
      "metadata": {
        "id": "_cSd9hACKabQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Model input')\n",
        "print(text_from_ids(seq_in).numpy()[0])\n",
        "print('Model output')\n",
        "print(text_from_ids(tf.argmax(tf.nn.softmax(model_out), axis=-1)).numpy()[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jukmn9DWKDdz",
        "outputId": "49f6266e-6b07-43f8-a6c6-1ebc71432c26"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model input\n",
            "b\":\\nCare keeps his watch in every old man's eye,\\nAnd where care lodges, sleep will never lie;\\nBut wher\"\n",
            "Model output\n",
            "b'OMNDDqBvqqLKKKLXKhh3,cK kWokq;CCyJEKHD EpKqCqq$XoL-h.LHLBE?hqBgJmgggoKKKqqKKh  EKkokq;HE omGg!3Kh.oH'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: It is important to sample from this distribution as taking the argmax of the distribution can easily get the model stuck in a loop."
      ],
      "metadata": {
        "id": "EsHWNxWlMobD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices = tf.random.categorical(logits=model_out[0], num_samples=1, dtype=tf.int32)\n",
        "sampled_indices = tf.squeeze(sampled_indices)"
      ],
      "metadata": {
        "id": "t4DSSWJ1MltB"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMiGvBtXOXG7",
        "outputId": "4c65d999-f54e-40a6-9450-2a77bf10e742"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(100,), dtype=int32, numpy=\n",
              "array([51, 23, 48, 11, 14, 13,  7, 50, 15,  6, 26,  2, 60,  4, 29, 40,  7,\n",
              "       32, 32, 33, 11, 42,  5, 33, 61, 53, 26, 28, 10, 57, 14,  5,  3, 15,\n",
              "       36, 49, 20, 14, 20, 10, 47,  8, 18, 31, 61, 32,  8, 28, 49, 29, 43,\n",
              "       31, 36, 54, 30, 62, 12, 60, 30, 14, 65, 20, 13, 51, 20, 45,  2, 41,\n",
              "       42, 17, 22, 36, 53, 12, 21, 53, 43, 23, 37, 31, 62, 52, 16, 16, 53,\n",
              "        0, 30, 31,  8, 19, 17,  3, 41, 63, 53, 58, 22, 39, 15, 58],\n",
              "      dtype=int32)>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text_from_ids(sampled_indices).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqfgsd5TOfB7",
        "outputId": "6b9b6999-c7ba-4225-e6ee-5a234b0cf04a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b\"wnZ$SY,X;!tEB.PK,II:$QT:xctVdkSTU;GyhShduLOoxILVyP oGisgCBsS3hYwhlENQzHGcCFc nmogeDDc[UNK]soL'zUNfc\\nH&;\\n\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the model\n",
        "At this point the problem can be treated as a standard classification problem. Given the previous RNN state, and the input this time step, predict the class of the next character."
      ],
      "metadata": {
        "id": "9A9nukVSO_dD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_model.compile(optimizer='adam',\n",
        "                 loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                 metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "R5qCxnqRPb7j"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configure checkpoints"
      ],
      "metadata": {
        "id": "kVOhS1pVQliJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir,'ckpt_{epoch}')\n",
        "\n",
        "cb = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix, save_weights_only=True)"
      ],
      "metadata": {
        "id": "_ypxPLwPPbwC"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 20\n",
        "my_model_history_1 = my_model.fit(dataset, epochs=EPOCHS, callbacks=[cb])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxGEvE76RC-9",
        "outputId": "55b026fd-b9a0-4456-8281-118f68eafb3e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "173/173 [==============================] - 31s 127ms/step - loss: 2.7339 - accuracy: 0.2794\n",
            "Epoch 2/20\n",
            "173/173 [==============================] - 23s 124ms/step - loss: 1.9924 - accuracy: 0.4175\n",
            "Epoch 3/20\n",
            "173/173 [==============================] - 23s 125ms/step - loss: 1.7153 - accuracy: 0.4916\n",
            "Epoch 4/20\n",
            "173/173 [==============================] - 23s 126ms/step - loss: 1.5538 - accuracy: 0.5340\n",
            "Epoch 5/20\n",
            "173/173 [==============================] - 23s 126ms/step - loss: 1.4542 - accuracy: 0.5600\n",
            "Epoch 6/20\n",
            "173/173 [==============================] - 23s 125ms/step - loss: 1.3853 - accuracy: 0.5769\n",
            "Epoch 7/20\n",
            "173/173 [==============================] - 23s 125ms/step - loss: 1.3331 - accuracy: 0.5899\n",
            "Epoch 8/20\n",
            "173/173 [==============================] - 23s 125ms/step - loss: 1.2880 - accuracy: 0.6015\n",
            "Epoch 9/20\n",
            "173/173 [==============================] - 23s 126ms/step - loss: 1.2465 - accuracy: 0.6120\n",
            "Epoch 10/20\n",
            "173/173 [==============================] - 23s 125ms/step - loss: 1.2077 - accuracy: 0.6224\n",
            "Epoch 11/20\n",
            "173/173 [==============================] - 23s 126ms/step - loss: 1.1671 - accuracy: 0.6332\n",
            "Epoch 12/20\n",
            "173/173 [==============================] - 23s 125ms/step - loss: 1.1256 - accuracy: 0.6451\n",
            "Epoch 13/20\n",
            "173/173 [==============================] - 23s 126ms/step - loss: 1.0825 - accuracy: 0.6578\n",
            "Epoch 14/20\n",
            "173/173 [==============================] - 23s 125ms/step - loss: 1.0363 - accuracy: 0.6713\n",
            "Epoch 15/20\n",
            "173/173 [==============================] - 23s 124ms/step - loss: 0.9873 - accuracy: 0.6864\n",
            "Epoch 16/20\n",
            "173/173 [==============================] - 23s 125ms/step - loss: 0.9362 - accuracy: 0.7027\n",
            "Epoch 17/20\n",
            "173/173 [==============================] - 23s 125ms/step - loss: 0.8838 - accuracy: 0.7193\n",
            "Epoch 18/20\n",
            "173/173 [==============================] - 23s 125ms/step - loss: 0.8307 - accuracy: 0.7360\n",
            "Epoch 19/20\n",
            "173/173 [==============================] - 24s 130ms/step - loss: 0.7796 - accuracy: 0.7530\n",
            "Epoch 20/20\n",
            "173/173 [==============================] - 26s 130ms/step - loss: 0.7298 - accuracy: 0.7691\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate text"
      ],
      "metadata": {
        "id": "5n2nf20ccyZx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# lets generate text using this trained model\n",
        "\n",
        "data = b'ROMEO:'\n",
        "id = tf.expand_dims(ids_from_chars(tf.strings.unicode_split(data,'UTF-8')), axis=0)\n",
        "new_state = None\n",
        "result = [data]\n",
        "for i in range(500):\n",
        "  # predict the next character and update the state\n",
        "  logits, new_state = my_model(inputs=id, return_state=True, states=new_state)\n",
        "  # Only use the last prediction.\n",
        "  logits = logits[:,-1,:]\n",
        "  # lookup the character\n",
        "  id = tf.random.categorical(logits,num_samples=1)\n",
        "  next_char = chars_from_ids(tf.squeeze(id,axis=-1))\n",
        "  result.append(next_char.numpy()[0])\n",
        "\n"
      ],
      "metadata": {
        "id": "ABvO-LMDUs9Q"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tf.strings.join(result).numpy().decode('utf-8'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C09Y4SrpV8mC",
        "outputId": "a1ebdd20-523e-4ca2-f556-a03d3ef8a856"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "The fear before, none--ink an old much.\n",
            "\n",
            "PETER:\n",
            "That's after Baptist to the rock Tarpeians:\n",
            "This night-conspire foolish week; and hadst not,\n",
            "For happier repute far win that's name; for\n",
            "when men alrest successions, he is dead,\n",
            "I could to do them good.\n",
            "\n",
            "LADY GREY:\n",
            "The friends but keeps a wife of ten too lamention.\n",
            "Come, yes, you shall now kept loar on my sacred life\n",
            "Hath hadness of sack men to nothing;\n",
            "In the high words with death. Hine's earth:\n",
            "Meaning, but what's good queen:\n",
            "The people's nose i\n"
          ]
        }
      ]
    }
  ]
}