{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Load Text.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOyOUK5wiAcg3unRSXpHIRt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ajay-user/DataScience/blob/master/Notes/Load_Text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuJ92CjdO0yE"
      },
      "source": [
        "## Example 1: Predict the tag for a Stack Overflow question\n",
        "\n",
        "Download a dataset of programming questions from Stack Overflow. Each question (\"How do I sort a dictionary by value?\") is labeled with exactly one tag (Python, CSharp, JavaScript, or Java).\n",
        "\n",
        "The task is to develop a model that predicts the tag for a question. This is an example of multi-class classification, an important and widely applicable kind of machine learning problem.\n",
        "\n",
        "**Download and explore the dataset**\n",
        "\n",
        "Next, you will download the dataset, and explore the directory structure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyEoBNhAOoxV"
      },
      "source": [
        "import re\n",
        "import string\n",
        "import pathlib\n",
        "import tensorflow as tf"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LonS-e3bPxIv",
        "outputId": "2ef8ba2a-d4bc-4ccc-b351-b59767815a45"
      },
      "source": [
        "data_url = 'https://storage.googleapis.com/download.tensorflow.org/data/stack_overflow_16k.tar.gz'\n",
        "\n",
        "dataset = tf.keras.utils.get_file(fname = 'stack_overflow',\n",
        "                                  origin = data_url,\n",
        "                                  cache_dir = 'stack_overflow',\n",
        "                                  cache_subdir = '',\n",
        "                                  untar = True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/stack_overflow_16k.tar.gz\n",
            "6053888/6053168 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gn4HAw7EQgbm",
        "outputId": "340012e8-3b38-4f86-c74f-acfa363597eb"
      },
      "source": [
        "print('Path to the downloaded file',dataset)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Path to the downloaded file /tmp/.keras/stack_overflow\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNpLc6tBQiNY",
        "outputId": "8f286390-f2ca-4090-eb25-28b380612285"
      },
      "source": [
        "dataset_dir = pathlib.Path(dataset).parent\n",
        "print('Parent',dataset_dir)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parent /tmp/.keras\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGXRryT5RH7i",
        "outputId": "cb833ae7-e6e6-4b58-e4fa-42768d641576"
      },
      "source": [
        "for directory in list(dataset_dir.iterdir()):\n",
        "  print(\"path :\",directory)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "path : /tmp/.keras/train\n",
            "path : /tmp/.keras/stack_overflow.tar.gz\n",
            "path : /tmp/.keras/test\n",
            "path : /tmp/.keras/README.md\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKT4KGCtRJYT",
        "outputId": "d01cb879-1e76-46a2-a29b-7e7553b8c90d"
      },
      "source": [
        "train_dir = dataset_dir/'train'\n",
        "test_dir = dataset_dir/'test'\n",
        "\n",
        "print('path to training data:',train_dir)\n",
        "print('path to testing data:',test_dir)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "path to training data: /tmp/.keras/train\n",
            "path to testing data: /tmp/.keras/test\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHSxCSqaSZB-",
        "outputId": "d091e038-5bab-4144-90ef-79b98bc4d6a8"
      },
      "source": [
        "for f in train_dir.iterdir():\n",
        "  print('training data:',f)\n",
        "\n",
        "print('-'*50)\n",
        "\n",
        "for f in test_dir.iterdir():\n",
        "  print('testing data:',f)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training data: /tmp/.keras/train/csharp\n",
            "training data: /tmp/.keras/train/python\n",
            "training data: /tmp/.keras/train/javascript\n",
            "training data: /tmp/.keras/train/java\n",
            "--------------------------------------------------\n",
            "testing data: /tmp/.keras/test/csharp\n",
            "testing data: /tmp/.keras/test/python\n",
            "testing data: /tmp/.keras/test/javascript\n",
            "testing data: /tmp/.keras/test/java\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vY3yqbcUbfG"
      },
      "source": [
        "The `train/csharp`, `train/java`, `train/python` and `train/javascript` directories contain many text files, each of which is a Stack Overflow question. Print a file and inspect the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "idzvMYJwT0aL",
        "outputId": "a4c2f3ce-179a-480c-c09a-c99f88f05484"
      },
      "source": [
        "\n",
        "for tag in ['csharp', 'python', 'javascript', 'java']:\n",
        "  path = train_dir/tag\n",
        "  for que in path.iterdir():\n",
        "    with open(que) as f:\n",
        "      print('Tag:',tag)\n",
        "      print(f.read())\n",
        "      break\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tag: csharp\n",
            "\"how can i keep invoking a method inside a button in blank i am trying to call a function (testdraw) repeatedly inside a button (btn_auto_update) in a blank windows application:..private void btn_auto_update_click(object sender, eventargs e).{      ..}..private void testdraw().{.    textoutput.text += \"\"drawingrn\"\";.}...or i need a way to do what this codeline in python do:..def auto_get(self):.    self.testdraw().    self.after(1000, self.auto_get)\"\n",
            "\n",
            "Tag: python\n",
            "\"how to correct this loop using an array if any keyword from an array is mentioned in the title, don't click the title. if the title doesn't mention any of the keywords click the title...right now it clicks all the time, and i know why but i don't know how to fix it. it always clicks because it goes through the whole array and eventually there is a keyword that is not in the title. ideas?..arr = [\"\"bunny\"\", \"\"watch\"\", \"\"book\"\"]..title = (\"\"the book of coding. (e-book) by seb tota\"\").lower().length = len(arr).for i in range(0,length - 1):.    if arr[i] in title:.        print \"\"dont click\"\".    else:.        print \"\"click\"\"...this should not click the title because arr[2] is in the title\"\n",
            "\n",
            "Tag: javascript\n",
            "\"blank giving nan error i am using the below codes:..&lt;script type=\"\"text/blank\"\"&gt;.function minus(){.var t=document.getelementbyid(\"\"totalcost\"\").value;.var u=document.getelementbyid(\"\"advpay\"\").value;  .var set=(t-u);.document.getelementbyid(\"\"textfield\"\").value=set;.return true;.}.&lt;/script&gt;...i entered \"\"6000\"\" as value in totalcost id field and \"\"1000\"\" in advpay id field. so in textfield id field, it should show 5000 (6000-1000), but it is giving answer as nan. where is the error ?\"\n",
            "\n",
            "Tag: java\n",
            "\"how to detect opened windows in blank by using the below code i opened a window(say some software installer window) it went fine,.here i want to check weather the window got opened or not...how to detect whether the window is opened or not?..code snippet:-..  cmdarray = \"\"......\"\";.  runtime runtime = runtime.getruntime(); .  process responce = runtime.exec(cmdarray);...what kind of window..ans:- installer anywhere window..at last i able to find the window is opened or not by using the solution provided by hussain..if i want to close the window, how i can close it.......i used taskkill /f /im &lt;&lt; installer_window>>.exe command in blank(like runtime.exe(\"\"command\"\");)to kill the process..but window is not getting closed. is there any way to close that window...thanks in advance...@hussain..i tried as you said, but i am getting error like error: the process \"\"proces.exe\"\" with pid 4408 could not be terminated reason: this process can only be terminated forcefully (with /f option). thats why i included /f, with the help of this i able to kill the process but window is not getting closed...please provide a quick solution, waiting for reply......thanks in advance\"\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PM2L6zjNYEI_"
      },
      "source": [
        "### Load the dataset\n",
        "Next, you will load the data off disk and prepare it into a format suitable for training. To do so, you will use `text_dataset_from_directory utility` to create a labeled `tf.data.Dataset`. \n",
        "\n",
        "**`tf.data`**, it's a powerful collection of tools for building input pipelines.\n",
        "\n",
        "The `preprocessing.text_dataset_from_directory` expects a directory structure as follows.\n",
        "\n",
        "<br>\n",
        "train/<br>\n",
        "...csharp/<br>\n",
        "......1.txt<br>\n",
        "......2.txt<br>\n",
        "...java/<br>\n",
        "......1.txt<br>\n",
        "......2.txt<br>\n",
        "...javascript/<br>\n",
        "......1.txt<br>\n",
        "......2.txt<br>\n",
        "...python/<br>\n",
        "......1.txt<br>\n",
        "......2.txt<br>\n",
        "<br>\n",
        "\n",
        "When running a machine learning experiment, it is a best practice to divide your dataset into three splits: train, validation, and test. The Stack Overflow dataset has already been divided into train and test, but it lacks a validation set. Create a validation set using an 80:20 split of the training data by using the validation_split argument below.\n",
        "\n",
        "**Note: When using the validation_split and subset arguments, make sure to either specify a random seed, or to pass shuffle=False, so that the validation and training splits have no overlap.**\n",
        "\n",
        "**The labels are 0, 1, 2 or 3. To see which of these correspond to which string label, you can check the class_names property on the dataset.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc9xGYodUwF_",
        "outputId": "d672f6a6-7351-49c0-bab7-56d7e9b82f60"
      },
      "source": [
        "batch_size = 32\n",
        "seed = 42\n",
        "\n",
        "raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory( directory = train_dir,\n",
        "                                                                  batch_size = batch_size,\n",
        "                                                                  seed = seed,\n",
        "                                                                  validation_split = 0.2,\n",
        "                                                                  subset = 'training')\n",
        "\n",
        "raw_val_ds = tf.keras.preprocessing.text_dataset_from_directory( directory = train_dir,\n",
        "                                                                  batch_size = batch_size,\n",
        "                                                                  seed = seed,\n",
        "                                                                  validation_split = 0.2,\n",
        "                                                                  subset = 'validation')\n",
        "\n",
        "raw_test_ds = tf.keras.preprocessing.text_dataset_from_directory( directory = test_dir,\n",
        "                                                                  batch_size = batch_size,\n",
        "                                                                  seed = seed)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 8000 files belonging to 4 classes.\n",
            "Using 6400 files for training.\n",
            "Found 8000 files belonging to 4 classes.\n",
            "Using 1600 files for validation.\n",
            "Found 8000 files belonging to 4 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8uTLfKWZ0Id"
      },
      "source": [
        "Note: To increase the difficulty of the classification problem, the dataset author replaced occurrences of the words Python, CSharp, JavaScript, or Java in the programming question with the word blank."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OoaNMis8ZQFF",
        "outputId": "035f827d-034c-4c39-83b4-5d8b4c6b64ca"
      },
      "source": [
        "batch_text , batch_label = next(iter(raw_train_ds))\n",
        "\n",
        "for i in range(5):\n",
        "  print(\"Question : \",batch_text[i])\n",
        "  print(\"Tag : \",batch_label[i])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question :  tf.Tensor(b'\"my tester is going to the wrong constructor i am new to programming so if i ask a question that can be easily fixed, please forgive me. my program has a tester class with a main. when i send that to my regularpolygon class, it sends it to the wrong constructor. i have two constructors. 1 without perameters..public regularpolygon().    {.       mynumsides = 5;.       mysidelength = 30;.    }//end default constructor...and my second, with perameters. ..public regularpolygon(int numsides, double sidelength).    {.        mynumsides = numsides;.        mysidelength = sidelength;.    }// end constructor...in my tester class i have these two lines:..regularpolygon shape = new regularpolygon(numsides, sidelength);.        shape.menu();...numsides and sidelength were declared and initialized earlier in the testing class...so what i want to happen, is the tester class sends numsides and sidelength to the second constructor and use it in that class. but it only uses the default constructor, which therefor ruins the whole rest of the program. can somebody help me?..for those of you who want to see more of my code: here you go..public double vertexangle().    {.        system.out.println(\"\"the vertex angle method: \"\" + mynumsides);// prints out 5.        system.out.println(\"\"the vertex angle method: \"\" + mysidelength); // prints out 30..        double vertexangle;.        vertexangle = ((mynumsides - 2.0) / mynumsides) * 180.0;.        return vertexangle;.    }//end method vertexangle..public void menu().{.    system.out.println(mynumsides); // prints out what the user puts in.    system.out.println(mysidelength); // prints out what the user puts in.    gotographic();.    calcr(mynumsides, mysidelength);.    calcr(mynumsides, mysidelength);.    print(); .}// end menu...this is my entire tester class:..public static void main(string[] arg).{.    int numsides;.    double sidelength;.    scanner keyboard = new scanner(system.in);..    system.out.println(\"\"welcome to the regular polygon program!\"\");.    system.out.println();..    system.out.print(\"\"enter the number of sides of the polygon ==&gt; \"\");.    numsides = keyboard.nextint();.    system.out.println();..    system.out.print(\"\"enter the side length of each side ==&gt; \"\");.    sidelength = keyboard.nextdouble();.    system.out.println();..    regularpolygon shape = new regularpolygon(numsides, sidelength);.    shape.menu();.}//end main...for testing it i sent it numsides 4 and sidelength 100.\"\\n', shape=(), dtype=string)\n",
            "Tag :  tf.Tensor(1, shape=(), dtype=int32)\n",
            "Question :  tf.Tensor(b'\"blank code slow skin detection this code changes the color space to lab and using a threshold finds the skin area of an image. but it\\'s ridiculously slow. i don\\'t know how to make it faster ?    ..from colormath.color_objects import *..def skindetection(img, treshold=80, color=[255,20,147]):..    print img.shape.    res=img.copy().    for x in range(img.shape[0]):.        for y in range(img.shape[1]):.            rgbimg=rgbcolor(img[x,y,0],img[x,y,1],img[x,y,2]).            labimg=rgbimg.convert_to(\\'lab\\', debug=false).            if (labimg.lab_l &gt; treshold):.                res[x,y,:]=color.            else: .                res[x,y,:]=img[x,y,:]..    return res\"\\n', shape=(), dtype=string)\n",
            "Tag :  tf.Tensor(3, shape=(), dtype=int32)\n",
            "Question :  tf.Tensor(b'\"option and validation in blank i want to add a new option on my system where i want to add two text files, both rental.txt and customer.txt. inside each text are id numbers of the customer, the videotape they need and the price...i want to place it as an option on my code. right now i have:...add customer.rent return.view list.search.exit...i want to add this as my sixth option. say for example i ordered a video, it would display the price and would let me confirm the price and if i am going to buy it or not...here is my current code:..  import blank.io.*;.    import blank.util.arraylist;.    import static blank.lang.system.out;..    public class rentalsystem{.    static bufferedreader input = new bufferedreader(new inputstreamreader(system.in));.    static file file = new file(\"\"file.txt\"\");.    static arraylist&lt;string&gt; list = new arraylist&lt;string&gt;();.    static int rows;..    public static void main(string[] args) throws exception{.        introduction();.        system.out.print(\"\"nn\"\");.        login();.        system.out.print(\"\"nnnnnnnnnnnnnnnnnnnnnn\"\");.        introduction();.        string repeat;.        do{.            loadfile();.            system.out.print(\"\"nwhat do you want to do?nn\"\");.            system.out.print(\"\"n                    - - - - - - - - - - - - - - - - - - - - - - -\"\");.            system.out.print(\"\"nn                    |     1. add customer    |   2. rent return |n\"\");.            system.out.print(\"\"n                    - - - - - - - - - - - - - - - - - - - - - - -\"\");.            system.out.print(\"\"nn                    |     3. view list       |   4. search      |n\"\");.            system.out.print(\"\"n                    - - - - - - - - - - - - - - - - - - - - - - -\"\");.            system.out.print(\"\"nn                                             |   5. exit        |n\"\");.            system.out.print(\"\"n                                              - - - - - - - - - -\"\");.            system.out.print(\"\"nnchoice:\"\");.            int choice = integer.parseint(input.readline());.            switch(choice){.                case 1:.                    writedata();.                    break;.                case 2:.                    rentdata();.                    break;.                case 3:.                    viewlist();.                    break;.                case 4:.                    search();.                    break;.                case 5:.                    system.out.println(\"\"goodbye!\"\");.                    system.exit(0);.                default:.                    system.out.print(\"\"invalid choice: \"\");.                    break;.            }.            system.out.print(\"\"ndo another task? [y/n] \"\");.            repeat = input.readline();.        }while(repeat.equals(\"\"y\"\"));..        if(repeat!=\"\"y\"\") system.out.println(\"\"ngoodbye!\"\");..    }..    public static void writedata() throws exception{.        system.out.print(\"\"nname: \"\");.        string cname = input.readline();.        system.out.print(\"\"address: \"\");.        string add = input.readline();.        system.out.print(\"\"phone no.: \"\");.        string pno = input.readline();.        system.out.print(\"\"rental amount: \"\");.        string ramount = input.readline();.        system.out.print(\"\"tapenumber: \"\");.        string tno = input.readline();.        system.out.print(\"\"title: \"\");.        string title = input.readline();.        system.out.print(\"\"date borrowed: \"\");.        string dborrowed = input.readline();.        system.out.print(\"\"due date: \"\");.        string ddate = input.readline();.        createline(cname, add, pno, ramount,tno, title, dborrowed, ddate);.        rentdata();.    }..    public static void createline(string name, string address, string phone , string rental, string tapenumber, string title, string borrowed, string due) throws exception{.        filewriter fw = new filewriter(file, true);.        fw.write(\"\"nname: \"\"+name + \"\"naddress: \"\" + address +\"\"nphone no.: \"\"+ phone+\"\"nrentalamount: \"\"+rental+\"\"ntape no.: \"\"+ tapenumber+\"\"ntitle: \"\"+ title+\"\"ndate borrowed: \"\"+borrowed +\"\"ndue date: \"\"+ due+\"\":rn\"\");.        fw.close();.    }..    public static void loadfile() throws exception{.        try{.            list.clear();.            fileinputstream fstream = new fileinputstream(file);.            bufferedreader br = new bufferedreader(new inputstreamreader(fstream));.            rows = 0;.            while( br.ready()).            {.                list.add(br.readline());.                rows++;.            }.            br.close();.        } catch(exception e){.            system.out.println(\"\"list not yet loaded.\"\");.        }.    }..    public static void viewlist(){.        system.out.print(\"\"n~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~\"\");.        system.out.print(\"\" |list of all costumers|\"\");.        system.out.print(\"\"~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~\"\");.        for(int i = 0; i &lt;rows; i++){.            system.out.println(list.get(i));.        }.    }.        public static void rentdata()throws exception.    {   system.out.print(\"\"n~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~\"\");.        system.out.print(\"\" |rent data list|\"\");.        system.out.print(\"\"~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~\"\");.        system.out.print(\"\"nenter customer name: \"\");.        string cname = input.readline();.        system.out.print(\"\"date borrowed: \"\");.        string dborrowed = input.readline();.        system.out.print(\"\"due date: \"\");.        string ddate = input.readline();.        system.out.print(\"\"return date: \"\");.        string rdate = input.readline();.        system.out.print(\"\"rent amount: \"\");.        string ramount = input.readline();..        system.out.print(\"\"you pay:\"\"+ramount);...    }.    public static void search()throws exception.    {   system.out.print(\"\"n~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~\"\");.        system.out.print(\"\" |search costumers|\"\");.        system.out.print(\"\"~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~\"\");.        system.out.print(\"\"nenter costumer name: \"\");.        string cname = input.readline();.        boolean found = false;..        for(int i=0; i &lt; rows; i++){.            string temp[] = list.get(i).split(\"\",\"\");..            if(cname.equals(temp[0])){.            system.out.println(\"\"search result:nyou are \"\" + temp[0] + \"\" from \"\" + temp[1] + \"\".\"\"+ temp[2] + \"\".\"\"+ temp[3] + \"\".\"\"+ temp[4] + \"\".\"\"+ temp[5] + \"\" is \"\" + temp[6] + \"\".\"\"+ temp[7] + \"\" is \"\" + temp[8] + \"\".\"\");.                found = true;.            }.        }..        if(!found){.            system.out.print(\"\"no results.\"\");.        }..    }..        public static boolean evaluate(string uname, string pass){.        if (uname.equals(\"\"admin\"\")&amp;&amp;pass.equals(\"\"12345\"\")) return true;.        else return false;.    }..    public static string login()throws exception{.        bufferedreader input=new bufferedreader(new inputstreamreader(system.in));.        int counter=0;.        do{.            system.out.print(\"\"username:\"\");.            string uname =input.readline();.            system.out.print(\"\"password:\"\");.            string pass =input.readline();..            boolean accept= evaluate(uname,pass);..            if(accept){.                break;.                }else{.                    system.out.println(\"\"incorrect username or password!\"\");.                    counter ++;.                    }.        }while(counter&lt;3);..            if(counter !=3) return \"\"login successful\"\";.            else return \"\"login failed\"\";.            }.        public static void introduction() throws exception{..        system.out.println(\"\"                  - - - - - - - - - - - - - - - - - - - - - - - - -\"\");.        system.out.println(\"\"                  !                  r e n t a l                  !\"\");.        system.out.println(\"\"                   ! ~ ~ ~ ~ ~ !  =================  ! ~ ~ ~ ~ ~ !\"\");.        system.out.println(\"\"                  !                  s y s t e m                  !\"\");.        system.out.println(\"\"                  - - - - - - - - - - - - - - - - - - - - - - - - -\"\");.        }..}\"\\n', shape=(), dtype=string)\n",
            "Tag :  tf.Tensor(1, shape=(), dtype=int32)\n",
            "Question :  tf.Tensor(b'\"exception: dynamic sql generation for the updatecommand is not supported against a selectcommand that does not return any key i dont know what is the problem this my code : ..string nomtable;..datatable listeetablissementtable = new datatable();.datatable listeinteretstable = new datatable();.dataset ds = new dataset();.sqldataadapter da;.sqlcommandbuilder cmdb;..private void listeinterets_click(object sender, eventargs e).{.    nomtable = \"\"listeinteretstable\"\";.    d.cnx.open();.    da = new sqldataadapter(\"\"select nome from offices\"\", d.cnx);.    ds = new dataset();.    da.fill(ds, nomtable);.    datagridview1.datasource = ds.tables[nomtable];.}..private void sauvgarder_click(object sender, eventargs e).{.    d.cnx.open();.    cmdb = new sqlcommandbuilder(da);.    da.update(ds, nomtable);.    d.cnx.close();.}\"\\n', shape=(), dtype=string)\n",
            "Tag :  tf.Tensor(0, shape=(), dtype=int32)\n",
            "Question :  tf.Tensor(b'\"parameter with question mark and super in blank, i\\'ve come across a method that is formatted like this:..public final subscription subscribe(final action1&lt;? super t&gt; onnext, final action1&lt;throwable&gt; onerror) {.}...in the first parameter, what does the question mark and super mean?\"\\n', shape=(), dtype=string)\n",
            "Tag :  tf.Tensor(1, shape=(), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G86uFz4MaUpU",
        "outputId": "195aa813-e776-4b1d-faf6-7648bd66bd60"
      },
      "source": [
        "for i, name in enumerate(raw_train_ds.class_names):\n",
        "  print('label:',i,'classname:',name)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "label: 0 classname: csharp\n",
            "label: 1 classname: java\n",
            "label: 2 classname: javascript\n",
            "label: 3 classname: python\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MR5qXWGEbJrj"
      },
      "source": [
        "### Prepare the dataset for training\n",
        "**Standardize**, **Tokenize**, and **Vectorize** the data using the `preprocessing.TextVectorization` layer.\n",
        "\n",
        "**Standardization** refers to preprocessing the text, typically to remove punctuation or HTML elements to simplify the dataset.\n",
        "\n",
        "**Tokenization** refers to splitting strings into tokens (for example, splitting a sentence into individual words by splitting on whitespace).\n",
        "\n",
        "**Vectorization** refers to converting tokens into numbers so they can be fed into a neural network.\n",
        "\n",
        "\n",
        "\n",
        "The default standardization converts text to lowercase and removes punctuation.\n",
        "\n",
        "The default tokenizer splits on whitespace.\n",
        "\n",
        "**`The default vectorization mode is int. This outputs integer indices (one per token). This mode can be used to build models that take word order into account. You can also use other modes, like binary, to build bag-of-word models`**.\n",
        "\n",
        "You will build two models to learn more about these. \n",
        "* First, you will use the binary model to build a bag-of-words model. \n",
        "* Next, you will use the int mode with a 1D ConvNet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOoIVaR-dQPU"
      },
      "source": [
        "### Vocab configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5K0DKsn8alxH"
      },
      "source": [
        "VOCAB_SIZE = 10000\n",
        "MAX_SEQUENCE_LENGTH = 250\n",
        "\n",
        "# pad_to_max_tokens\tOnly valid in \"binary\", \"count\", and \"tf-idf\" modes.\n",
        "# If True, the output will have its feature axis padded to max_tokens even if the number of unique tokens in the vocabulary is less than max_tokens, \n",
        "# resulting in a tensor of shape [batch_size, max_tokens] regardless of vocabulary size. \n",
        "# Defaults to False.\n",
        "\n",
        "# 'output_sequence_length'\tOnly valid in INT mode.\n",
        "# If set, the output will have its time dimension padded or truncated to exactly output_sequence_length values, \n",
        "# resulting in a tensor of shape [batch_size, output_sequence_length] regardless of how many tokens resulted from the splitting step.\n",
        "# Defaults to None."
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jvz-GcTdNrN"
      },
      "source": [
        "binary_vectorize_layer = tf.keras.layers.experimental.preprocessing.TextVectorization(max_tokens = VOCAB_SIZE,\n",
        "                                                                                      output_mode = 'binary')"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OP8ZVGQfOd9"
      },
      "source": [
        "For int mode, in addition to maximum vocabulary size, you need to set an explicit maximum sequence length, which will cause the layer to pad or truncate sequences to exactly sequence_length values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iY-s2bELfFht"
      },
      "source": [
        "int_vectorize_layer = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
        "    max_tokens = VOCAB_SIZE,\n",
        "     output_mode = 'int',\n",
        "      output_sequence_length = MAX_SEQUENCE_LENGTH)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EZzp51lfpWF"
      },
      "source": [
        "Next, you will call adapt to fit the state of the preprocessing layer to the dataset. This will cause the model to build an index of strings to integers.\n",
        "\n",
        "Note: it's important to only use your training data when calling adapt (using the test set would leak information).\n",
        "\n",
        "**Make a text-only dataset (without labels), then call adapt**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eo5DrfOjfggE"
      },
      "source": [
        "#  text-only dataset\n",
        "text_ds = raw_train_ds.map(lambda text,label:text)\n",
        "\n",
        "#  calling adapt on text-only dataset\n",
        "binary_vectorize_layer.adapt(text_ds)\n",
        "int_vectorize_layer.adapt(text_ds)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhAbbxQDgqN7"
      },
      "source": [
        "See the result of using these layers to preprocess data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVUZ8xOfgGjY"
      },
      "source": [
        "def binary_vectorize_text(text,label):\n",
        "  text = tf.expand_dims(text, -1)\n",
        "  return binary_vectorize_layer(text), label\n",
        "\n",
        "def int_vectorize_text(text,label):\n",
        "  text = tf.expand_dims(text, -1)\n",
        "  return int_vectorize_layer(text), label"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfCuvhE4hEKG"
      },
      "source": [
        "sample_text = batch_text[0]\n",
        "sample_label = batch_label[0]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSbFJft-hOkl",
        "outputId": "78b1b013-bc64-400f-b1a3-2903da05a912"
      },
      "source": [
        "print('Sample text before Vectorization \\n',sample_text)\n",
        "print('Sample label:',sample_label)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample text before Vectorization \n",
            " tf.Tensor(b'\"my tester is going to the wrong constructor i am new to programming so if i ask a question that can be easily fixed, please forgive me. my program has a tester class with a main. when i send that to my regularpolygon class, it sends it to the wrong constructor. i have two constructors. 1 without perameters..public regularpolygon().    {.       mynumsides = 5;.       mysidelength = 30;.    }//end default constructor...and my second, with perameters. ..public regularpolygon(int numsides, double sidelength).    {.        mynumsides = numsides;.        mysidelength = sidelength;.    }// end constructor...in my tester class i have these two lines:..regularpolygon shape = new regularpolygon(numsides, sidelength);.        shape.menu();...numsides and sidelength were declared and initialized earlier in the testing class...so what i want to happen, is the tester class sends numsides and sidelength to the second constructor and use it in that class. but it only uses the default constructor, which therefor ruins the whole rest of the program. can somebody help me?..for those of you who want to see more of my code: here you go..public double vertexangle().    {.        system.out.println(\"\"the vertex angle method: \"\" + mynumsides);// prints out 5.        system.out.println(\"\"the vertex angle method: \"\" + mysidelength); // prints out 30..        double vertexangle;.        vertexangle = ((mynumsides - 2.0) / mynumsides) * 180.0;.        return vertexangle;.    }//end method vertexangle..public void menu().{.    system.out.println(mynumsides); // prints out what the user puts in.    system.out.println(mysidelength); // prints out what the user puts in.    gotographic();.    calcr(mynumsides, mysidelength);.    calcr(mynumsides, mysidelength);.    print(); .}// end menu...this is my entire tester class:..public static void main(string[] arg).{.    int numsides;.    double sidelength;.    scanner keyboard = new scanner(system.in);..    system.out.println(\"\"welcome to the regular polygon program!\"\");.    system.out.println();..    system.out.print(\"\"enter the number of sides of the polygon ==&gt; \"\");.    numsides = keyboard.nextint();.    system.out.println();..    system.out.print(\"\"enter the side length of each side ==&gt; \"\");.    sidelength = keyboard.nextdouble();.    system.out.println();..    regularpolygon shape = new regularpolygon(numsides, sidelength);.    shape.menu();.}//end main...for testing it i sent it numsides 4 and sidelength 100.\"\\n', shape=(), dtype=string)\n",
            "Sample label: tf.Tensor(1, shape=(), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FeXANxoqhWUT",
        "outputId": "fc51cccc-f2e1-491e-82e7-4d4a192e16da"
      },
      "source": [
        "print('Binary vectorization \\n',binary_vectorize_text(sample_text,sample_label))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Binary vectorization \n",
            " (<tf.Tensor: shape=(1, 10000), dtype=float32, numpy=array([[1., 1., 1., ..., 0., 0., 0.]], dtype=float32)>, <tf.Tensor: shape=(), dtype=int32, numpy=1>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihbgCpvih21k",
        "outputId": "db3e05fd-3514-4704-8973-a567ae55c8f5"
      },
      "source": [
        "print('Integer vectorization \\n',int_vectorize_text(sample_text,sample_label))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Integer vectorization \n",
            " (<tf.Tensor: shape=(1, 250), dtype=int64, numpy=\n",
            "array([[  23, 1978,    6,  414,    4,    2,  151,  314,    3,   34,   15,\n",
            "           4,  598,   50,   10,    3,  675,    5,  159,   14,   35,   33,\n",
            "        2146, 1180,  160, 5800,   74,   23,   86,   95,    5, 1978,   29,\n",
            "          21,    5,  153,   44,    3,  448,   14,    4,   23,    1,   29,\n",
            "          11, 1845,   11,    4,    2,  151,  314,    3,   17,  121, 2205,\n",
            "          25,  203,    1,    1, 7557,  145, 7555,  473,  197,  369,    1,\n",
            "          23,  199,   21,    1,   22,    1, 6398,  120, 4485, 7557, 6398,\n",
            "        7555, 4485,  197,    1,   23, 1978,   29,    3,   17,  229,  121,\n",
            "           1, 2242,   15,    1, 4485,    1,    8, 4485,  541, 1082,    8,\n",
            "        1369, 2070,    7,    2,  773,    1,   55,    3,   46,    4, 1078,\n",
            "           6,    2, 1978,   29, 1845, 6398,    8, 4485,    4,    2,  199,\n",
            "         314,    8,   70,   11,    7,   14,   29,   26,   11,   93,  722,\n",
            "           2,  369,  314,   66,    1,    1,    2,  635,  835,    9,    2,\n",
            "          86,   35, 1801,  104,    1,  432,    9,   58,  874,   46,    4,\n",
            "         189,  181,    9,   23,   30,  101,   58,    1,  120, 8479,  982,\n",
            "        2887, 2629,   64, 7557,  514,   94,  145,  982, 2887, 2629,   64,\n",
            "        7555,  514,   94,  473,  120, 8479, 8479, 7557,  324, 7557, 4847,\n",
            "          27, 8479,  197,   64,    1,   42,  666,    1,  514,   94,   55,\n",
            "           2,   99, 2920,    7,    1,  514,   94,   55,    2,   99, 2920,\n",
            "           7,    1,    1, 7555,    1, 7555,   75,  197,    1,    6,   23,\n",
            "         935, 1978, 1145,   53,   42,  170, 1696,   28, 6398,  120, 4485,\n",
            "         256, 1260,   15,  453, 4923,    4,    2,  940, 4535,   86,  371,\n",
            "        1173,    2,   68,    9, 3042,    9,    2, 4535]])>, <tf.Tensor: shape=(), dtype=int32, numpy=1>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9igQI7FWiWOl"
      },
      "source": [
        "As you can see above, binary mode returns an array denoting which tokens exist at least once in the input, while int mode replaces each token by an integer, thus preserving their order. You can lookup the token (string) that each integer corresponds to by calling `.get_vocabulary()` on the layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7MQR3n-h7LK",
        "outputId": "e44bdcd8-b625-436c-84be-7e4bd73387a5"
      },
      "source": [
        "for i in int_vectorize_text(sample_text, sample_label)[0].numpy()[0]:\n",
        "  print(int_vectorize_layer.get_vocabulary()[i], end=' ')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "my tester is going to the wrong constructor i am new to programming so if i ask a question that can be easily fixed please forgive me my program has a tester class with a main when i send that to my [UNK] class it sends it to the wrong constructor i have two constructors 1 without [UNK] [UNK] mynumsides 5 mysidelength 30 end default [UNK] my second with [UNK] public [UNK] numsides double sidelength mynumsides numsides mysidelength sidelength end [UNK] my tester class i have these two [UNK] shape new [UNK] sidelength [UNK] and sidelength were declared and initialized earlier in the testing [UNK] what i want to happen is the tester class sends numsides and sidelength to the second constructor and use it in that class but it only uses the default constructor which [UNK] [UNK] the whole rest of the program can somebody help [UNK] those of you who want to see more of my code here you [UNK] double vertexangle systemoutprintlnthe vertex angle method mynumsides prints out 5 systemoutprintlnthe vertex angle method mysidelength prints out 30 double vertexangle vertexangle mynumsides 20 mynumsides 1800 return vertexangle end method [UNK] void menu [UNK] prints out what the user puts in [UNK] prints out what the user puts in [UNK] [UNK] mysidelength [UNK] mysidelength print end [UNK] is my entire tester classpublic static void mainstring arg int numsides double sidelength scanner keyboard new scannersystemin systemoutprintlnwelcome to the regular polygon program systemoutprintln systemoutprintenter the number of sides of the polygon "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0Mj76-vkFyD"
      },
      "source": [
        "**Apply the TextVectorization layers you created earlier to the train, validation, and test dataset**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VV5GYESUjMhs"
      },
      "source": [
        "binary_train_ds = raw_train_ds.map(binary_vectorize_text)\n",
        "binary_val_ds = raw_val_ds.map(binary_vectorize_text)\n",
        "binary_test_ds = raw_test_ds.map(binary_vectorize_text)\n",
        "\n",
        "int_train_ds = raw_train_ds.map(int_vectorize_text)\n",
        "int_val_ds = raw_val_ds.map(int_vectorize_text)\n",
        "int_test_ds = raw_test_ds.map(int_vectorize_text)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVgLIQu0liQn"
      },
      "source": [
        "**Configure the dataset for performance** \n",
        "\n",
        "These are two important methods you should use when loading data to make sure that I/O does not become blocking.\n",
        "\n",
        "**.cache()** keeps data in memory after it's loaded off disk. This will ensure the dataset does not become a bottleneck while training your model. If your dataset is too large to fit into memory, you can also use this method to create a performant on-disk cache, which is more efficient to read than many small files.\n",
        "\n",
        "**.prefetch()** overlaps data preprocessing and model execution while training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "un0y9WXllhID"
      },
      "source": [
        "def config_for_performance(ds):\n",
        "  ds = ds.cache()\n",
        "  ds = ds.prefetch(tf.data.AUTOTUNE)\n",
        "  return ds"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNeB5ANVmL_F"
      },
      "source": [
        "binary_train_ds = config_for_performance(binary_train_ds)\n",
        "binary_val_ds = config_for_performance(binary_val_ds)\n",
        "binary_test_ds = config_for_performance(binary_test_ds)\n",
        "\n",
        "int_train_ds = config_for_performance(int_train_ds)\n",
        "int_val_ds = config_for_performance(int_val_ds)\n",
        "int_test_ds = config_for_performance(int_test_ds)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1Og0EHHm4aR"
      },
      "source": [
        "### Create a Model\n",
        "it's time to create our neural network. For the binary vectorized data, **train a simple bag-of-words linear model**:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXEP32GHPnCe"
      },
      "source": [
        "#### Binary Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZs90uSBmyvJ"
      },
      "source": [
        "binary_model = tf.keras.Sequential([\n",
        "                                    tf.keras.layers.Dense(units=4)\n",
        "])"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZAuvuoiLHlu"
      },
      "source": [
        "binary_model.compile(optimizer='adam',\n",
        "                     loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                     metrics=['accuracy'])"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PA_vFpOwMDrR",
        "outputId": "9a9bbbfd-467a-4a08-cc91-b76b793332db"
      },
      "source": [
        "binary_model.fit(x=binary_train_ds, epochs=10, validation_data=binary_val_ds)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "200/200 [==============================] - 4s 18ms/step - loss: 1.1131 - accuracy: 0.6567 - val_loss: 0.9100 - val_accuracy: 0.7744\n",
            "Epoch 2/10\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 0.7752 - accuracy: 0.8209 - val_loss: 0.7475 - val_accuracy: 0.8000\n",
            "Epoch 3/10\n",
            "200/200 [==============================] - 1s 4ms/step - loss: 0.6251 - accuracy: 0.8598 - val_loss: 0.6624 - val_accuracy: 0.8094\n",
            "Epoch 4/10\n",
            "200/200 [==============================] - 1s 4ms/step - loss: 0.5323 - accuracy: 0.8866 - val_loss: 0.6093 - val_accuracy: 0.8200\n",
            "Epoch 5/10\n",
            "200/200 [==============================] - 1s 4ms/step - loss: 0.4667 - accuracy: 0.9053 - val_loss: 0.5727 - val_accuracy: 0.8281\n",
            "Epoch 6/10\n",
            "200/200 [==============================] - 1s 4ms/step - loss: 0.4167 - accuracy: 0.9167 - val_loss: 0.5462 - val_accuracy: 0.8388\n",
            "Epoch 7/10\n",
            "200/200 [==============================] - 1s 4ms/step - loss: 0.3767 - accuracy: 0.9261 - val_loss: 0.5261 - val_accuracy: 0.8400\n",
            "Epoch 8/10\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 0.3436 - accuracy: 0.9362 - val_loss: 0.5106 - val_accuracy: 0.8394\n",
            "Epoch 9/10\n",
            "200/200 [==============================] - 1s 4ms/step - loss: 0.3155 - accuracy: 0.9420 - val_loss: 0.4984 - val_accuracy: 0.8413\n",
            "Epoch 10/10\n",
            "200/200 [==============================] - 1s 4ms/step - loss: 0.2912 - accuracy: 0.9484 - val_loss: 0.4887 - val_accuracy: 0.8394\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f093bf41a50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7N9VJJVKNedW"
      },
      "source": [
        "Next, you will use the int vectorized layer to build a 1D ConvNet.\n",
        "### ConvNet model template"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uanGOc9lNbqi"
      },
      "source": [
        "def create_model(vocab_size, num_labels):\n",
        "  model = tf.keras.Sequential([\n",
        "                                 tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=64, mask_zero=True),\n",
        "                                 tf.keras.layers.Conv1D(64, 5, strides=2, padding='valid', activation='relu'),\n",
        "                                 tf.keras.layers.GlobalAveragePooling1D(),\n",
        "                                 tf.keras.layers.Dense(num_labels)])\n",
        "  return model"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2ez0sytPu1I"
      },
      "source": [
        "#### Integer model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBc2xRsePAA1",
        "outputId": "06ca54c1-52a4-4216-9da8-8070509f89ec"
      },
      "source": [
        "# vocab size = 1 + VOCAB_SIZE since 0 is used additionally for padding.\n",
        "int_model = create_model(VOCAB_SIZE+1, 4)\n",
        "int_model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
        "int_model.fit(x = int_train_ds, epochs=5, validation_data=int_val_ds)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "200/200 [==============================] - 9s 41ms/step - loss: 1.2361 - accuracy: 0.4727 - val_loss: 0.8803 - val_accuracy: 0.6931\n",
            "Epoch 2/5\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.7120 - accuracy: 0.7431 - val_loss: 0.6093 - val_accuracy: 0.7881\n",
            "Epoch 3/5\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.4766 - accuracy: 0.8413 - val_loss: 0.5341 - val_accuracy: 0.8106\n",
            "Epoch 4/5\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.3367 - accuracy: 0.8972 - val_loss: 0.5153 - val_accuracy: 0.8125\n",
            "Epoch 5/5\n",
            "200/200 [==============================] - 6s 29ms/step - loss: 0.2405 - accuracy: 0.9352 - val_loss: 0.5267 - val_accuracy: 0.8056\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f093bc252d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIa8wuaPRTof"
      },
      "source": [
        "### Compare the models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1kzfbB8P6o6",
        "outputId": "b3be921e-192b-4a28-d6e8-7a74333130ae"
      },
      "source": [
        "binary_loss, binary_accuracy = binary_model.evaluate(binary_test_ds)\n",
        "print('Binary model accuray',binary_accuracy,'loss',binary_loss)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "250/250 [==============================] - 3s 11ms/step - loss: 0.5171 - accuracy: 0.8163\n",
            "Binary model accuray 0.8162500262260437 loss 0.517124593257904\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xE2iJY01R1zY",
        "outputId": "1e8a58f2-6f26-4e6e-c3c5-a3b7339e6a2d"
      },
      "source": [
        "int_loss, int_accuracy = int_model.evaluate(int_test_ds)\n",
        "print('Integer model accuray',int_accuracy,'loss',int_loss)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "250/250 [==============================] - 4s 15ms/step - loss: 0.5958 - accuracy: 0.7859\n",
            "Integer model accuray 0.7858750224113464 loss 0.5957909226417542\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wR7SVevSZuA"
      },
      "source": [
        "### Export the model\n",
        "In the code above, you applied the TextVectorization layer to the dataset before feeding text to the model. If you want to make your model capable of processing raw strings (for example, to simplify deploying it), you can include the TextVectorization layer inside your model. To do so, you can create a new model using the weights you just trained."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqUhqvtOSCE1"
      },
      "source": [
        "export_model = tf.keras.Sequential([\n",
        "                                    binary_vectorize_layer,\n",
        "                                    binary_model,\n",
        "                                    tf.keras.layers.Softmax()\n",
        "])"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGqlXIknUMc0"
      },
      "source": [
        "export_model.compile(optimizer='adam',\n",
        "                     loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "                     metrics=['accuracy'])"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8ca3eUTVBe4",
        "outputId": "d8d53a77-b535-475d-e0db-d013644fa063"
      },
      "source": [
        "# Test it with `raw_test_ds`, which yields raw strings\n",
        "loss, accuracy = export_model.evaluate(raw_test_ds)\n",
        "print('Export model accuracy',accuracy)\n",
        "print('Export model loss',loss)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "250/250 [==============================] - 3s 12ms/step - loss: 0.5171 - accuracy: 0.8163\n",
            "Export model accuracy 0.8162500262260437\n",
            "Export model loss 0.5171244740486145\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yop9uxnWV9Rr"
      },
      "source": [
        "### Run inference on new data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rnKe1GsVG_t"
      },
      "source": [
        "inputs = [\n",
        "    \"how do I extract keys from a dict into a list?\",  # python\n",
        "    \"debug public static void main(string[] args) {...}\",  # java\n",
        "]"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrUEzzXcWCaL"
      },
      "source": [
        "def get_string_labels(predicted_score):\n",
        "  int_labels = tf.argmax(predicted_score, axis=1)\n",
        "  return tf.gather(raw_train_ds.class_names, int_labels)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRLcUVXUWdpJ"
      },
      "source": [
        "predictions = export_model.predict(inputs)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_RaH2TLWh7z"
      },
      "source": [
        "output_labels = get_string_labels(predictions)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3gu4rUFWkhI",
        "outputId": "6a7bf6f7-18c0-43ca-c544-09961a37ae28"
      },
      "source": [
        "for i, o in zip(inputs, output_labels.numpy()):\n",
        "  print('Question',i)\n",
        "  print('Tag',o)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question how do I extract keys from a dict into a list?\n",
            "Tag b'python'\n",
            "Question debug public static void main(string[] args) {...}\n",
            "Tag b'java'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkMu3aVca7Yp"
      },
      "source": [
        "There is a performance difference to keep in mind when choosing where to apply your TextVectorization layer. Using it outside of your model enables you to do asynchronous CPU processing and buffering of your data when training on GPU. So, if you're training your model on the GPU, you probably want to go with this option to get the best performance while developing your model, then switch to including the TextVectorization layer inside your model when you're ready to prepare for deployment.\n",
        "\n",
        "## Example 2: Predict the author of Illiad translations\n",
        "The following provides an example of using `tf.data.TextLineDataset` to load examples from text files, and `tf.text` to preprocess the data. In this example, you will use three different English translations of the same work, Homer's Illiad, and train a model to identify the translator given a single line of text.\n",
        "\n",
        "**Download and explore the dataset**\n",
        "\n",
        "The texts of the three translations are by:\n",
        "\n",
        "* William Cowper — text\n",
        "\n",
        "* Edward, Earl of Derby — text\n",
        "\n",
        "* Samuel Butler — text\n",
        "\n",
        "The text files used in this tutorial have undergone some typical preprocessing tasks like removing document header and footer, line numbers and chapter titles. Download these lightly munged files locally."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vMo5tIefW3jC",
        "outputId": "0b06060f-67a6-4977-b1fc-25497f8ada7a"
      },
      "source": [
        "DIRECTORY_URL = 'https://storage.googleapis.com/download.tensorflow.org/data/illiad/'\n",
        "FILE_NAMES = ['cowper.txt', 'derby.txt', 'butler.txt']\n",
        "\n",
        "text_dir = ''\n",
        "for name in FILE_NAMES:\n",
        "  text_dir = tf.keras.utils.get_file(fname=name,origin=DIRECTORY_URL+name)\n",
        "  "
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/illiad/cowper.txt\n",
            "819200/815980 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/illiad/derby.txt\n",
            "811008/809730 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/illiad/butler.txt\n",
            "811008/807992 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jlVBN5pXNXf",
        "outputId": "f8dd90e5-845f-4a1f-d4e0-affe45093b78"
      },
      "source": [
        "import pathlib\n",
        "text_dir = pathlib.Path(text_dir)\n",
        "parent_dir = text_dir.parent\n",
        "print('Parent directory:',parent_dir)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parent directory: /root/.keras/datasets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Abb3VNDVchb3",
        "outputId": "df07ad08-8b8e-4878-e35f-2d5ce36a1d1e"
      },
      "source": [
        "text_files = []\n",
        "for item in parent_dir.iterdir():\n",
        "  text_files.append(item)\n",
        "  print('Text File:',item)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text File: /root/.keras/datasets/cowper.txt\n",
            "Text File: /root/.keras/datasets/butler.txt\n",
            "Text File: /root/.keras/datasets/derby.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zTXeS9dhF0X"
      },
      "source": [
        "### Load the dataset\n",
        "You will use `TextLineDataset`, which is designed to create a `tf.data.Dataset` from a text file in which each example is a line of text from the original file, whereas `text_dataset_from_directory` treats all contents of a file as a single example. `TextLineDataset` is useful for text data that is primarily line-based (for example, poetry or error logs).\n",
        "\n",
        "Iterate through these files, loading each one into its own dataset. Each example needs to be individually labeled, so use tf.data.Dataset.map to apply a labeler function to each one. This will iterate over every example in the dataset, returning (example, label) pairs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxhmDsJQdMAz"
      },
      "source": [
        "labeled_text_dataset = []\n",
        "\n",
        "def labeler(example, index):\n",
        "  return example, tf.cast(index, dtype=tf.int64)\n",
        "\n",
        "for index, file in enumerate(text_files):\n",
        "  text_line_ds = tf.data.TextLineDataset(file)\n",
        "  labeled_ds = text_line_ds.map(lambda text:labeler(text, index))\n",
        "  labeled_text_dataset.append(labeled_ds)\n",
        "\n",
        "\n",
        "  "
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvCbSTB6nKoc"
      },
      "source": [
        "Next, you'll combine these labeled datasets into a single dataset, and shuffle it.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDZTRCephBYm"
      },
      "source": [
        "all_labeled_data = labeled_text_dataset[0]\n",
        "\n",
        "for ds in labeled_text_dataset[1:]:\n",
        "  all_labeled_data = all_labeled_data.concatenate(ds)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSUFFg04n_Sg"
      },
      "source": [
        "### Dataset configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyLdW_0vmZj_"
      },
      "source": [
        "BUFFER_SIZE = 50000\n",
        "BATCH_SIZE = 64\n",
        "VALIDATION_SIZE = 5000"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpDHr_dqrflc"
      },
      "source": [
        "Shuffle the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwVD3OvkoDmS"
      },
      "source": [
        "all_labeled_data = all_labeled_data.shuffle(buffer_size=BUFFER_SIZE, reshuffle_each_iteration=False)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVeQLxgNrj30"
      },
      "source": [
        "Print out a few examples as before. The dataset hasn't been batched yet, hence each entry in `all_labeled_data ` corresponds to one data point:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVXRyF8Qp46n",
        "outputId": "e362072f-b7cc-45d2-a7d4-ae8e82c51af5"
      },
      "source": [
        "for data, label in all_labeled_data.take(10):\n",
        "  print('Sentence',data)\n",
        "  print('Label',label)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentence tf.Tensor(b'Found him, and such as none could waft aside,', shape=(), dtype=string)\n",
            "Label tf.Tensor(0, shape=(), dtype=int64)\n",
            "Sentence tf.Tensor(b\"Of fierce \\xc3\\x86acides. And now they reach'd\", shape=(), dtype=string)\n",
            "Label tf.Tensor(0, shape=(), dtype=int64)\n",
            "Sentence tf.Tensor(b\"By art averted Peleus' son; the form\", shape=(), dtype=string)\n",
            "Label tf.Tensor(0, shape=(), dtype=int64)\n",
            "Sentence tf.Tensor(b'Nor horse have I, nor car on which to mount;', shape=(), dtype=string)\n",
            "Label tf.Tensor(2, shape=(), dtype=int64)\n",
            "Sentence tf.Tensor(b'So swiftly past the eager horses flew.\"', shape=(), dtype=string)\n",
            "Label tf.Tensor(2, shape=(), dtype=int64)\n",
            "Sentence tf.Tensor(b\"Then Hector Leitus, Aloctryon's son,\", shape=(), dtype=string)\n",
            "Label tf.Tensor(2, shape=(), dtype=int64)\n",
            "Sentence tf.Tensor(b'There, settling by degrees, it rolls no more;', shape=(), dtype=string)\n",
            "Label tf.Tensor(0, shape=(), dtype=int64)\n",
            "Sentence tf.Tensor(b\"Supine, his eyes with pitchy darkness veil'd,\", shape=(), dtype=string)\n",
            "Label tf.Tensor(0, shape=(), dtype=int64)\n",
            "Sentence tf.Tensor(b'Then thou, Achilles, reverence the Gods;', shape=(), dtype=string)\n",
            "Label tf.Tensor(2, shape=(), dtype=int64)\n",
            "Sentence tf.Tensor(b'First he spoke to the two Ajaxes, who were doing their best already,', shape=(), dtype=string)\n",
            "Label tf.Tensor(1, shape=(), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "un9paXlO4d27"
      },
      "source": [
        "### Prepare the dataset for training\n",
        "Instead of using the Keras `TextVectorization` layer to preprocess our text dataset, you will now use the `tf.text` API to standardize and tokenize the data, build a vocabulary and use `StaticVocabularyTable` to map tokens to integers to feed to the model.\n",
        "\n",
        "While `tf.text` provides various tokenizers, you will use the `UnicodeScriptTokenizer` to tokenize our dataset.\n",
        "\n",
        "Define a function to convert the text to lower-case and tokenize it. You will use `tf.data.Dataset.map` to apply the tokenization to the dataset.\n",
        "\n",
        "A Tokenizer is a text.Splitter that splits strings into tokens. Tokens generally correspond to short substrings of the source string. Tokens can be encoded using either strings or integer ids (where integer ids could be created by hashing strings or by looking them up in a fixed vocabulary table that maps strings to ids).\n",
        "\n",
        "**`UnicodeScriptTokenizer`**\n",
        "\n",
        "Tokenizes UTF-8 by splitting when there is a change in Unicode script.\n",
        "\n",
        "The strings are split when successive tokens change their Unicode script or change being whitespace or not.\n",
        "\n",
        "By default, this tokenizer leaves out scripts matching the whitespace unicode property (use the keep_whitespace argument to keep it), so in this case the results are similar to the WhitespaceTokenizer. Any punctuation will get its own token (since it is in a different script), and any script change in the input string will be the location of a split.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0snJclwagdQ",
        "outputId": "626aa9ad-d5f2-4535-cbc5-da42f3575727"
      },
      "source": [
        "pip install tensorflow-text-nightly"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-text-nightly\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0b/62/e9f54b2c360920e35be40976d660f5735a72870d739124b83e72110065a5/tensorflow_text_nightly-2.7.0.dev20210626-cp37-cp37m-manylinux1_x86_64.whl (4.3MB)\n",
            "\u001b[K     |████████████████████████████████| 4.3MB 8.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text-nightly) (0.12.0)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-hub>=0.8.0->tensorflow-text-nightly) (1.19.5)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-hub>=0.8.0->tensorflow-text-nightly) (3.12.4)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorflow-hub>=0.8.0->tensorflow-text-nightly) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorflow-hub>=0.8.0->tensorflow-text-nightly) (57.0.0)\n",
            "Installing collected packages: tensorflow-text-nightly\n",
            "Successfully installed tensorflow-text-nightly-2.7.0.dev20210626\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsXeM_zo3YUU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8063fbfb-653f-4dae-8fcb-926a7a47892e"
      },
      "source": [
        "pip install -q tensorflow-text"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 4.3MB 7.3MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AsWbjpJK3dkd"
      },
      "source": [
        "import tensorflow_text "
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_y0YFbKi3g9Z"
      },
      "source": [
        "tokenizer = tensorflow_text.UnicodeScriptTokenizer()"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjJoy-o5bwSK"
      },
      "source": [
        "Define a function to convert the text to lower-case and tokenize it. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L79P8JTZbIPl"
      },
      "source": [
        "def tokenize(text, unused_label):\n",
        "  lowercase = tensorflow_text.case_fold_utf8(text)\n",
        "  return tokenizer.tokenize(lowercase)"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orMTCTiGmRoE",
        "outputId": "e776e568-741c-4ae0-ed47-7ed23ec7f24e"
      },
      "source": [
        "# eg: applying tokenize\n",
        "example_text, example_label = next(iter(all_labeled_data))\n",
        "print('Sample before tokenization \\n',example_text)\n",
        "print('Sample after tokenization \\n',tokenize(example_text, example_label))"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample before tokenization \n",
            " tf.Tensor(b'Found him, and such as none could waft aside,', shape=(), dtype=string)\n",
            "Sample after tokenization \n",
            " tf.Tensor(\n",
            "[b'found' b'him' b',' b'and' b'such' b'as' b'none' b'could' b'waft'\n",
            " b'aside' b','], shape=(11,), dtype=string)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjZtNRPeb0Fz"
      },
      "source": [
        "You will use tf.data.Dataset.map to apply the tokenization to the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8o4BfXlBbj0e"
      },
      "source": [
        "tokenized_ds = all_labeled_data.map(tokenize)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mujorPVMcFkF"
      },
      "source": [
        "You can iterate over the dataset and print out a few tokenized examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SB8WUhLb8MT",
        "outputId": "7d16c922-5e30-48a0-f0fc-31ac5cc25f2b"
      },
      "source": [
        "for toks in tokenized_ds.take(10):\n",
        "  print(toks)\n",
        " "
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[b'found' b'him' b',' b'and' b'such' b'as' b'none' b'could' b'waft'\n",
            " b'aside' b','], shape=(11,), dtype=string)\n",
            "tf.Tensor(\n",
            "[b'of' b'fierce' b'\\xc3\\xa6acides' b'.' b'and' b'now' b'they' b'reach'\n",
            " b\"'\" b'd'], shape=(10,), dtype=string)\n",
            "tf.Tensor([b'by' b'art' b'averted' b'peleus' b\"'\" b'son' b';' b'the' b'form'], shape=(9,), dtype=string)\n",
            "tf.Tensor(\n",
            "[b'nor' b'horse' b'have' b'i' b',' b'nor' b'car' b'on' b'which' b'to'\n",
            " b'mount' b';'], shape=(12,), dtype=string)\n",
            "tf.Tensor([b'so' b'swiftly' b'past' b'the' b'eager' b'horses' b'flew' b'.\"'], shape=(8,), dtype=string)\n",
            "tf.Tensor([b'then' b'hector' b'leitus' b',' b'aloctryon' b\"'\" b's' b'son' b','], shape=(9,), dtype=string)\n",
            "tf.Tensor(\n",
            "[b'there' b',' b'settling' b'by' b'degrees' b',' b'it' b'rolls' b'no'\n",
            " b'more' b';'], shape=(11,), dtype=string)\n",
            "tf.Tensor(\n",
            "[b'supine' b',' b'his' b'eyes' b'with' b'pitchy' b'darkness' b'veil' b\"'\"\n",
            " b'd' b','], shape=(11,), dtype=string)\n",
            "tf.Tensor([b'then' b'thou' b',' b'achilles' b',' b'reverence' b'the' b'gods' b';'], shape=(9,), dtype=string)\n",
            "tf.Tensor(\n",
            "[b'first' b'he' b'spoke' b'to' b'the' b'two' b'ajaxes' b',' b'who' b'were'\n",
            " b'doing' b'their' b'best' b'already' b','], shape=(15,), dtype=string)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFuAfefidZJi"
      },
      "source": [
        "Next, you will build a vocabulary by sorting tokens by frequency and keeping the top VOCAB_SIZE tokens.\n",
        "\n",
        "Dictionary in Python is an unordered collection of data values.\n",
        "Sometimes, when the KeyError is raised, it might become a problem. To overcome this Python introduces another dictionary like container known as `Defaultdict` which is present inside the `collections module`.\n",
        "\n",
        "Defaultdict is a container like dictionaries present in the module collections. Defaultdict is a sub-class of the dict class that returns a dictionary-like object. The functionality of both dictionaries and defualtdict are almost same except for the fact that defualtdict never raises a KeyError. It provides a default value for the key that does not exists.\n",
        "\n",
        "Syntax: `defaultdict`(`default_factory`)\n",
        "\n",
        "Parameters:\n",
        "\n",
        "`default_factory`: A function returning the default value for the dictionary defined. If this argument is absent then the dictionary raises a KeyError."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_PPiaFWcRC2"
      },
      "source": [
        "import collections"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YddA7gICedOq"
      },
      "source": [
        "def config_for_performance(ds):\n",
        "  ds = ds.cache()\n",
        "  ds = ds.prefetch(tf.data.AUTOTUNE)\n",
        "  return ds"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfvhZSAFe6KZ"
      },
      "source": [
        "tokenized_ds = config_for_performance(tokenized_ds)"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xliVc7PUfCIH"
      },
      "source": [
        "vocab_dict = collections.defaultdict(lambda : 0)\n",
        "\n",
        "\n",
        "for toks in tokenized_ds.as_numpy_iterator():\n",
        "  for tok in toks:\n",
        "    vocab_dict[tok] += 1"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0RfPFjHfXI3"
      },
      "source": [
        "# sort decending\n",
        "vocab = sorted(vocab_dict.items(), key=lambda x:x[1], reverse=True)"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xE8M-ndgWE0"
      },
      "source": [
        "# limit the vocab to VOCAB_SIZE = 10000\n",
        "vocab = [token for token, count in vocab]\n",
        "\n",
        "vocab = vocab[:VOCAB_SIZE]"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWkRjxF8gXGN",
        "outputId": "42a7c413-69b4-489a-bbfa-959ebb63b8db"
      },
      "source": [
        "print('Length of vocab ',len(vocab))\n",
        "print(\"First five vocab entries:\", vocab[:5])"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of vocab  10000\n",
            "First five vocab entries: [b',', b'the', b'and', b\"'\", b'of']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohnGVQx0hzDi"
      },
      "source": [
        "To convert the tokens into integers, use the vocab set to create a `StaticVocabularyTable`. \n",
        "\n",
        "You will map tokens to integers in the range [`2`, `vocab_size` +`2`]. As with the TextVectorization layer, `0` is reserved to denote padding and `1` is reserved to denote an out-of-vocabulary (OOV) token.\n",
        "\n",
        "`tf.lookup.StaticVocabularyTable` \n",
        "*  Raises **ValueError** when num_oov_buckets is not positive.\n",
        "*  Raises **TypeError**\twhen lookup_key_dtype or initializer.key_dtype are not integer or string. Also when initializer.value_dtype != int64."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFfe2nn6hoE2"
      },
      "source": [
        "keys = vocab\n",
        "values = range(2,len(vocab)+2)"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tI-H_etyjgUM"
      },
      "source": [
        "init = tf.lookup.KeyValueTensorInitializer(keys=keys,\n",
        "                                           values = values,\n",
        "                                           key_dtype = tf.string,\n",
        "                                           value_dtype = tf.int64)"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aILQE8xFlBes"
      },
      "source": [
        "# vocab look-up table\n",
        "\n",
        "vocab_table = tf.lookup.StaticVocabularyTable(initializer = init,\n",
        "                                              num_oov_buckets = 1)"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfuxYy6GldZI"
      },
      "source": [
        "Finally, define a fuction to **standardize**, **tokenize** and **vectorize** the dataset using the tokenizer and lookup table:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QU6VPbGlXiL"
      },
      "source": [
        "def preprocess_text(text, label):\n",
        "  standradize = tensorflow_text.case_fold_utf8(text)\n",
        "  tokenized = tokenizer.tokenize(standradize)\n",
        "  vectorized = vocab_table.lookup(tokenized)\n",
        "  return vectorized, label\n"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wW1CkYiUn9Lv"
      },
      "source": [
        "You can try this on a single example to see the output:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIJl3cnZn52r",
        "outputId": "fae09d6d-f2b4-45cd-b61d-bc71058c2eca"
      },
      "source": [
        "example_text, example_label = next(iter(all_labeled_data))\n",
        "print('Sample before preprocessing \\n', example_text)\n",
        "print('Sample after preprocessing \\n', preprocess_text(example_text, example_label))"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample before preprocessing \n",
            " tf.Tensor(b'Found him, and such as none could waft aside,', shape=(), dtype=string)\n",
            "Sample after preprocessing \n",
            " (<tf.Tensor: shape=(11,), dtype=int64, numpy=array([ 290,   16,    2,    4,  103,   25,  251,  201, 5028,  775,    2])>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYDQPxY_pV94"
      },
      "source": [
        "Now run the preprocess function on the dataset using tf.data.Dataset.map."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icJvBb09oUOj"
      },
      "source": [
        "all_encoded_data = all_labeled_data.map(preprocess_text)"
      ],
      "execution_count": 317,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-_vcW7op2Ma"
      },
      "source": [
        "### Split the dataset into train and test\n",
        "The Keras `TextVectorization` layer also batches and pads the vectorized data. \n",
        "\n",
        "**Padding** is required because the examples inside of a batch need to be the same size and shape, but the examples in these datasets are not all the same size — each line of text has a different number of words. `tf.data.Dataset` supports splitting and padded-batching datasets:\n",
        "\n",
        "The `tf.data.Dataset.padded_batch()` method allows you to specify padded_shapes for each component (`feature`) of the resulting batch. For example, if your input dataset is called `ds`:\n",
        "\n",
        "<br>\n",
        "padded_ds = `ds`.`padded_batch`(<br>\n",
        "    BATCH_SIZE,<br>\n",
        "    padded_shapes= {<br>\n",
        "        'label': [ ],                       # Scalar elements, no padding.<br>\n",
        "        'sequence_feature': [None],          # Vector elements, padded to longest.<br>\n",
        "        'seq_of_seqs_feature': [None, None],  # Matrix elements, padded to longest in each dimension.<br>\n",
        "    })     <br>                                  \n",
        "    <br>\n",
        "    \n",
        "Notice that the padded_shapes argument has the same structure as your input dataset's elements, so in this case it takes a dictionary with keys that match your feature names."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UlGdQb8pddc"
      },
      "source": [
        "# VALIDATION_SIZE = 5000\n",
        "\n",
        "train_data = all_encoded_data.skip(VALIDATION_SIZE).shuffle(BUFFER_SIZE)\n",
        "validation_data = all_encoded_data.take(VALIDATION_SIZE)"
      ],
      "execution_count": 318,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bEK_56JqMji"
      },
      "source": [
        "# padding \n",
        "train_data = train_data.padded_batch(batch_size=BATCH_SIZE)\n",
        "validation_data = validation_data.padded_batch(batch_size=BATCH_SIZE)"
      ],
      "execution_count": 319,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuXBcSbbtWf7"
      },
      "source": [
        "Now, validation_data and train_data are not collections of (example, label) pairs, but collections of batches. Each batch is a pair of (many examples, many labels) represented as arrays. To illustrate:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dt9L5c2JtIzS"
      },
      "source": [
        "example_text_batch , example_label_batch = next(iter(validation_data))"
      ],
      "execution_count": 320,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQeWWy4ptrE6",
        "outputId": "13e75f00-541e-41c3-8de0-bff705d6415b"
      },
      "source": [
        "print('Shape of text batch',example_text_batch.shape)\n",
        "print('Shape of label batch',example_label_batch.shape)\n",
        "print('first text example',example_text_batch[0])\n",
        "print('first label example',example_label_batch[0])"
      ],
      "execution_count": 321,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of text batch (64, 17)\n",
            "Shape of label batch (64,)\n",
            "first text example tf.Tensor(\n",
            "[ 290   16    2    4  103   25  251  201 5028  775    2    0    0    0\n",
            "    0    0    0], shape=(17,), dtype=int64)\n",
            "first label example tf.Tensor(0, shape=(), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKUdxIVQuQCz"
      },
      "source": [
        "Since we use `0` for padding and `1` for out-of-vocabulary (OOV) tokens, the vocabulary size has increased by two.\n",
        "\n",
        "Configure the datasets for better performance as before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFqrCJFRt28S"
      },
      "source": [
        "train_data = config_for_performance(train_data)\n",
        "validation_data = config_for_performance(validation_data)"
      ],
      "execution_count": 322,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-j4UsdKmun1D"
      },
      "source": [
        "### Train the model\n",
        "You can train a model on this dataset as before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qrdah1HPukL1"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "                             tf.keras.layers.Embedding(input_dim=VOCAB_SIZE+2, output_dim=64, mask_zero=True),\n",
        "                             tf.keras.layers.Conv1D(64, 5, strides=2, padding='valid', activation='relu'),\n",
        "                             tf.keras.layers.GlobalAveragePooling1D(),\n",
        "                             tf.keras.layers.Dense(3)                       \n",
        "])"
      ],
      "execution_count": 343,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGZu9eKovaxJ"
      },
      "source": [
        "model.compile(\n",
        "    optimizer = 'adam',\n",
        "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics = ['accuracy']\n",
        "             )"
      ],
      "execution_count": 344,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5etDpc4vcdC",
        "outputId": "dcf27218-7be2-42b1-e7f3-d81cd0f0761d"
      },
      "source": [
        "model.fit(x=train_data, epochs=3, validation_data=validation_data)"
      ],
      "execution_count": 345,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "697/697 [==============================] - 16s 22ms/step - loss: 0.5158 - accuracy: 0.7781 - val_loss: 0.3776 - val_accuracy: 0.8416\n",
            "Epoch 2/3\n",
            "697/697 [==============================] - 16s 23ms/step - loss: 0.3225 - accuracy: 0.8708 - val_loss: 0.3692 - val_accuracy: 0.8440\n",
            "Epoch 3/3\n",
            "697/697 [==============================] - 16s 22ms/step - loss: 0.2560 - accuracy: 0.8999 - val_loss: 0.3875 - val_accuracy: 0.8454\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f09352399d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 345
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79v9A1KNwcUP",
        "outputId": "46974778-8bc4-401f-d79f-79398e2424aa"
      },
      "source": [
        "loss, accuracy = model.evaluate(validation_data)\n",
        "print('Loss',loss)\n",
        "print('Model Accuracy',accuracy)"
      ],
      "execution_count": 346,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "79/79 [==============================] - 2s 3ms/step - loss: 0.3875 - accuracy: 0.8454\n",
            "Loss 0.38751453161239624\n",
            "Model Accuracy 0.8453999757766724\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qtr9038ZyGVK"
      },
      "source": [
        "### Export the model\n",
        "To make our model capable to taking raw strings as input, you will create a TextVectorization layer that performs the same steps as our custom preprocessing function. Since you already trained a vocabulary, you can use set_vocaublary instead of adapt which trains a new vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LdVOT9axk8L"
      },
      "source": [
        "preprocess_layer = tf.keras.layers.experimental.preprocessing.TextVectorization(max_tokens=VOCAB_SIZE+2,\n",
        "                                                                                standardize=tensorflow_text.case_fold_utf8,\n",
        "                                                                                split=tokenizer.tokenize,\n",
        "                                                                                output_mode='int',\n",
        "                                                                                output_sequence_length=20)"
      ],
      "execution_count": 361,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCNNY5XmyqZ6"
      },
      "source": [
        "preprocess_layer.set_vocabulary(vocab)"
      ],
      "execution_count": 362,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SAujrnjyyYA"
      },
      "source": [
        "export_model = tf.keras.Sequential([\n",
        "                                    preprocess_layer,\n",
        "                                    model,\n",
        "                                    tf.keras.layers.Activation('sigmoid'),\n",
        "])"
      ],
      "execution_count": 363,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFzqC7gjy6GT"
      },
      "source": [
        "export_model.compile(optimizer='adam',\n",
        "                     loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "                     metrics=['accuracy'])"
      ],
      "execution_count": 364,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jf1YNnCEzpjA"
      },
      "source": [
        "# Create a test dataset of raw strings\n",
        "test_data = all_labeled_data.take(VALIDATION_SIZE).batch(BATCH_SIZE)\n",
        "test_data = config_for_performance(test_data)"
      ],
      "execution_count": 365,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7OiAb5b0H82",
        "outputId": "6f01cb4e-4f38-444b-d68c-668fdb9dc9dd"
      },
      "source": [
        "loss,accuracy = export_model.evaluate(test_data)\n",
        "print('Loss',loss)\n",
        "print('Accuracy',accuracy)"
      ],
      "execution_count": 366,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "79/79 [==============================] - 8s 4ms/step - loss: 0.3880 - accuracy: 0.8362\n",
            "Loss 0.3879798948764801\n",
            "Accuracy 0.8361999988555908\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ip9QD84M-quH"
      },
      "source": [
        "### Run inference on new data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocL5yah_8vfW"
      },
      "source": [
        "inputs = [\n",
        "    \"Join'd to th' Ionians with their flowing robes,\",  # Label: 1\n",
        "    \"the allies, and his armour flashed about him so that he seemed to all\",  # Label: 2\n",
        "    \"And with loud clangor of his arms he fell.\",  # Label: 0\n",
        "]"
      ],
      "execution_count": 367,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkO6jXXh-zPb",
        "outputId": "b90e5a15-889e-4f5a-84c0-b195d7129f55"
      },
      "source": [
        "predictions = export_model.predict(inputs)\n",
        "outputs = tf.argmax(predictions, axis=1)\n",
        "\n",
        "for i, o in zip(inputs, outputs):\n",
        "  print('Line :',i)\n",
        "  print('Label :',o)"
      ],
      "execution_count": 368,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Line : Join'd to th' Ionians with their flowing robes,\n",
            "Label : tf.Tensor(2, shape=(), dtype=int64)\n",
            "Line : the allies, and his armour flashed about him so that he seemed to all\n",
            "Label : tf.Tensor(1, shape=(), dtype=int64)\n",
            "Line : And with loud clangor of his arms he fell.\n",
            "Label : tf.Tensor(0, shape=(), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hD6fmGiiJnz2"
      },
      "source": [
        "## Downloading more datasets using TensorFlow Datasets (TFDS)\n",
        "You can download many more datasets from TensorFlow Datasets. As an example, you will download the IMDB Large Movie Review dataset, and use it to train a model for sentiment classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BbaJ2EjGIXP"
      },
      "source": [
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": 264,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0HJmnJGGJ96"
      },
      "source": [
        "train_ds = tfds.load(name='imdb_reviews',\n",
        "                     split='train[:80%]',\n",
        "                     batch_size=BATCH_SIZE,\n",
        "                     shuffle_files=True,\n",
        "                     as_supervised=True\n",
        "                     )\n",
        "\n",
        "val_ds = tfds.load(name='imdb_reviews',\n",
        "                     split='train[80%:]',\n",
        "                     batch_size=BATCH_SIZE,\n",
        "                     shuffle_files=True,\n",
        "                     as_supervised=True\n",
        "                     )"
      ],
      "execution_count": 270,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUrU1nd8LQHW",
        "outputId": "cd738bcb-bac0-4780-cc27-6d312d8bc633"
      },
      "source": [
        "for batch_text, batch_label in val_ds.take(1):\n",
        "  for i in range(5):\n",
        "    print('Review:',batch_text[i])\n",
        "    print('Label:',batch_label[i])"
      ],
      "execution_count": 271,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Review: tf.Tensor(b\"Instead, go to the zoo, buy some peanuts and feed 'em to the monkeys. Monkeys are funny. People with amnesia who don't say much, just sit there with vacant eyes are not all that funny.<br /><br />Black comedy? There isn't a black person in it, and there isn't one funny thing in it either.<br /><br />Walmart buys these things up somehow and puts them on their dollar rack. It's labeled Unrated. I think they took out the topless scene. They may have taken out other stuff too, who knows? All we know is that whatever they took out, isn't there any more.<br /><br />The acting seemed OK to me. There's a lot of unfathomables tho. It's supposed to be a city? It's supposed to be a big lake? If it's so hot in the church people are fanning themselves, why are they all wearing coats?\", shape=(), dtype=string)\n",
            "Label: tf.Tensor(0, shape=(), dtype=int64)\n",
            "Review: tf.Tensor(b'Well, was Morgan Freeman any more unusual as God than George Burns? This film sure was better than that bore, \"Oh, God\". I was totally engrossed and LMAO all the way through. Carrey was perfect as the out of sorts anchorman wannabe, and Aniston carried off her part as the frustrated girlfriend in her usual well played performance. I, for one, don\\'t consider her to be either ugly or untalented. I think my favorite scene was when Carrey opened up the file cabinet thinking it could never hold his life history. See if you can spot the file in the cabinet that holds the events of his bathroom humor: I was rolling over this one. Well written and even better played out, this comedy will go down as one of this funnyman\\'s best.', shape=(), dtype=string)\n",
            "Label: tf.Tensor(1, shape=(), dtype=int64)\n",
            "Review: tf.Tensor(b'I remember stumbling upon this special while channel-surfing in 1965. I had never heard of Barbra before. When the show was over, I thought \"This is probably the best thing on TV I will ever see in my life.\" 42 years later, that has held true. There is still nothing so amazing, so honestly astonishing as the talent that was displayed here. You can talk about all the super-stars you want to, this is the most superlative of them all!<br /><br />You name it, she can do it. Comedy, pathos, sultry seduction, ballads, Barbra is truly a story-teller. Her ability to pull off anything she attempts is legendary. But this special was made in the beginning, and helped to create the legend that she quickly became. In spite of rising so far in such a short time, she has fulfilled the promise, revealing more of her talents as she went along. But they are all here from the very beginning. You will not be disappointed in viewing this.', shape=(), dtype=string)\n",
            "Label: tf.Tensor(1, shape=(), dtype=int64)\n",
            "Review: tf.Tensor(b\"Firstly, I would like to point out that people who have criticised this film have made some glaring errors. Anything that has a rating below 6/10 is clearly utter nonsense.<br /><br />Creep is an absolutely fantastic film with amazing film effects. The actors are highly believable, the narrative thought provoking and the horror and graphical content extremely disturbing. <br /><br />There is much mystique in this film. Many questions arise as the audience are revealed to the strange and freakish creature that makes habitat in the dark rat ridden tunnels. How was 'Craig' created and what happened to him?<br /><br />A fantastic film with a large chill factor. A film with so many unanswered questions and a film that needs to be appreciated along with others like 28 Days Later, The Bunker, Dog Soldiers and Deathwatch.<br /><br />Look forward to more of these fantastic films!!\", shape=(), dtype=string)\n",
            "Label: tf.Tensor(1, shape=(), dtype=int64)\n",
            "Review: tf.Tensor(b\"I'm sorry but I didn't like this doc very much. I can think of a million ways it could have been better. The people who made it obviously don't have much imagination. The interviews aren't very interesting and no real insight is offered. The footage isn't assembled in a very informative way, either. It's too bad because this is a movie that really deserves spellbinding special features. One thing I'll say is that Isabella Rosselini gets more beautiful the older she gets. All considered, this only gets a '4.'\", shape=(), dtype=string)\n",
            "Label: tf.Tensor(0, shape=(), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TROPjltjM_gd"
      },
      "source": [
        "You can now preprocess the data and train a model as before.\n",
        "\n",
        "Note: You will use losses.BinaryCrossentropy instead of losses.SparseCategoricalCrossentropy for your model since this is a binary classification problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZWtH1jkMHIz"
      },
      "source": [
        "vectorize_layer = tf.keras.layers.experimental.preprocessing.TextVectorization(max_tokens = VOCAB_SIZE,\n",
        "                                                                               output_mode = 'int',\n",
        "                                                                               output_sequence_length = MAX_SEQUENCE_LENGTH)"
      ],
      "execution_count": 273,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLJjvU2NPn5U"
      },
      "source": [
        "# Make a text-only dataset (without labels), then call adapt\n",
        "text_ds = train_ds.map(lambda x,y:x)\n",
        "\n",
        "vectorize_layer.adapt(text_ds)"
      ],
      "execution_count": 277,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZyIVZ30P11W"
      },
      "source": [
        "def vectorize_text(text, label):\n",
        "  text = tf.expand_dims(text,-1)\n",
        "  return vectorize_layer(text), label"
      ],
      "execution_count": 278,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQNUKgF9TKtH"
      },
      "source": [
        "train_ds = train_ds.map(vectorize_text)\n",
        "val_ds = val_ds.map(vectorize_text)"
      ],
      "execution_count": 285,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_RoBSgNP4hh"
      },
      "source": [
        "# Configure datasets for performance as before\n",
        "train_ds = config_for_performance(train_ds)\n",
        "val_ds = config_for_performance(val_ds)"
      ],
      "execution_count": 286,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXOyhINOQs36"
      },
      "source": [
        "### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8qFGqBJQkmC"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "                             tf.keras.layers.Embedding(input_dim=VOCAB_SIZE+1, output_dim=64, mask_zero=True),\n",
        "                             tf.keras.layers.Conv1D(64, 5, strides=2, padding='valid', activation='relu'),\n",
        "                             tf.keras.layers.GlobalAveragePooling1D(),\n",
        "                             tf.keras.layers.Dense(1)\n",
        "])"
      ],
      "execution_count": 287,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRWMbPjbSFvL"
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 288,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKZL83egSaOj",
        "outputId": "88dff930-7fee-4ad1-c964-697f085f4052"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 289,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_23\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_10 (Embedding)     (None, None, 64)          640064    \n",
            "_________________________________________________________________\n",
            "conv1d_7 (Conv1D)            (None, None, 64)          20544     \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d_7 ( (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 660,673\n",
            "Trainable params: 660,673\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BespobqtScMd",
        "outputId": "c9bb04f9-3245-4efc-dcc6-7d1869888582"
      },
      "source": [
        "model.fit(x=train_ds, validation_data=val_ds, epochs=3)"
      ],
      "execution_count": 290,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "313/313 [==============================] - 22s 69ms/step - loss: 0.4667 - accuracy: 0.7271 - val_loss: 0.3179 - val_accuracy: 0.8550\n",
            "Epoch 2/3\n",
            "313/313 [==============================] - 17s 53ms/step - loss: 0.2391 - accuracy: 0.9013 - val_loss: 0.3104 - val_accuracy: 0.8686\n",
            "Epoch 3/3\n",
            "313/313 [==============================] - 17s 53ms/step - loss: 0.1748 - accuracy: 0.9331 - val_loss: 0.3392 - val_accuracy: 0.8662\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f0920f6ffd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 290
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqpBBM_6Squx",
        "outputId": "a95bf4e9-8fd6-470a-ce3d-d7f531d077e9"
      },
      "source": [
        "loss, accuracy = model.evaluate(val_ds)\n",
        "\n",
        "print(\"Loss: \", loss)\n",
        "print(\"Accuracy: {:2.2%}\".format(accuracy))"
      ],
      "execution_count": 291,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "79/79 [==============================] - 1s 13ms/step - loss: 0.3392 - accuracy: 0.8662\n",
            "Loss:  0.33923861384391785\n",
            "Accuracy: 86.62%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zhp8oxB8TyHd"
      },
      "source": [
        "### Export the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2ZorkI8TxkN"
      },
      "source": [
        "export_model = tf.keras.Sequential([\n",
        "                                    vectorize_layer,\n",
        "                                    model,\n",
        "                                    tf.keras.layers.Activation('sigmoid')\n",
        "])"
      ],
      "execution_count": 292,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0o4sod69UH_V"
      },
      "source": [
        "export_model.compile(optimizer='adam',\n",
        "                     loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
        "                     metrics=['accuracy'])"
      ],
      "execution_count": 293,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZgHMa69UWni",
        "outputId": "31de358f-e05f-4fa4-cc81-7945e488d232"
      },
      "source": [
        "# 0 --> negative review\n",
        "# 1 --> positive review\n",
        "inputs = [\n",
        "    \"This is a fantastic movie.\",\n",
        "    \"This is a bad movie.\",\n",
        "    \"This movie was so bad that it was good.\",\n",
        "    \"I will never say yes to watching this movie.\",\n",
        "]\n",
        "\n",
        "predictions = export_model.predict(inputs)\n",
        "outputs = [round(pred[0]) for pred in predictions]\n",
        "for i, o in zip(inputs, outputs):\n",
        "  print('Review:',i)\n",
        "  print('Label:',o)"
      ],
      "execution_count": 303,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Review: This is a fantastic movie.\n",
            "Label: 1\n",
            "Review: This is a bad movie.\n",
            "Label: 0\n",
            "Review: This movie was so bad that it was good.\n",
            "Label: 0\n",
            "Review: I will never say yes to watching this movie.\n",
            "Label: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPmv7rwPYvKo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}